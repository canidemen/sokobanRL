{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sokoban RL Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\canid\\OneDrive\\Masaüstü\\Python\\CS_175\\sokobanRL\\sokobanRL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\canid\\OneDrive\\Masaüstü\\Python\\CS_175\\sokobanRL\\.venv\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is A45C-F748\n",
      "\n",
      " Directory of c:\\Users\\canid\\OneDrive\\Masa�st�\\Python\\CS_175\\sokobanRL\\sokobanRL\n",
      "\n",
      "12/05/2025  02:57 PM    <DIR>          .\n",
      "12/05/2025  02:15 PM    <DIR>          ..\n",
      "12/03/2025  07:28 PM               952 check_import_by_can.py\n",
      "12/05/2025  02:14 PM    <DIR>          checkpoints\n",
      "12/05/2025  02:14 PM    <DIR>          logs\n",
      "12/03/2025  06:25 PM                 8 README.md\n",
      "12/05/2025  02:58 PM                86 requirements.txt\n",
      "12/05/2025  03:08 PM    <DIR>          src\n",
      "12/05/2025  02:08 PM    <DIR>          test\n",
      "               3 File(s)          1,046 bytes\n",
      "               6 Dir(s)  63,032,307,712 bytes free\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (2.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (2.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (3.10.7)\n",
      "Requirement already satisfied: gymnasium in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (1.2.2)\n",
      "Requirement already satisfied: gym-sokoban in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (0.0.6)\n",
      "Requirement already satisfied: pygame in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (2.6.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2025.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from gymnasium->-r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from gymnasium->-r requirements.txt (line 5)) (0.0.4)\n",
      "Requirement already satisfied: gym>=0.2.3 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from gym-sokoban->-r requirements.txt (line 6)) (0.26.2)\n",
      "Requirement already satisfied: tqdm>=4.32.1 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from gym-sokoban->-r requirements.txt (line 6)) (4.67.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from gym-sokoban->-r requirements.txt (line 6)) (2.37.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from gym-sokoban->-r requirements.txt (line 6)) (2.32.5)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from gym>=0.2.3->gym-sokoban->-r requirements.txt (line 6)) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from requests>=2.22.0->gym-sokoban->-r requirements.txt (line 6)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from requests>=2.22.0->gym-sokoban->-r requirements.txt (line 6)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from requests>=2.22.0->gym-sokoban->-r requirements.txt (line 6)) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from requests>=2.22.0->gym-sokoban->-r requirements.txt (line 6)) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from tqdm>=4.32.1->gym-sokoban->-r requirements.txt (line 6)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\canid\\onedrive\\masaüstü\\python\\cs_175\\sokobanrl\\.venv\\lib\\site-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 1)) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%ls\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_sokoban\n",
    "import pygame\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Convolutional layers for processing the game state\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        # Actor head (policy)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "        \n",
    "        # Critic head (value function)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.actor(conv_out), self.critic(conv_out)\n",
    "    \n",
    "    def get_action_probs(self, x):\n",
    "        logits, _ = self.forward(x)\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        _, value = self.forward(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, env, lr=3e-4, gamma=0.99, eps_clip=0.2, K_epochs=4, gae_lambda=0.95):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        \n",
    "        # Get observation shape\n",
    "        obs = env.reset()\n",
    "        if len(obs.shape) == 3:\n",
    "            obs = np.transpose(obs, (2, 0, 1))\n",
    "        \n",
    "        self.input_shape = obs.shape\n",
    "        self.n_actions = env.action_space.n\n",
    "        \n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.policy = ActorCritic(self.input_shape, self.n_actions).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        self.policy_old = ActorCritic(self.input_shape, self.n_actions).to(self.device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            action_probs = self.policy_old.get_action_probs(state)\n",
    "        \n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), action_logprob.item()\n",
    "    \n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = values[t + 1]\n",
    "            \n",
    "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        return advantages\n",
    "    \n",
    "    def update(self, memory):\n",
    "        states = torch.FloatTensor(np.array(memory['states'])).to(self.device)\n",
    "        actions = torch.LongTensor(memory['actions']).to(self.device)\n",
    "        old_logprobs = torch.FloatTensor(memory['logprobs']).to(self.device)\n",
    "        \n",
    "        rewards = memory['rewards']\n",
    "        dones = memory['dones']\n",
    "        \n",
    "        # Compute values and advantages\n",
    "        with torch.no_grad():\n",
    "            values = self.policy_old.get_value(states).squeeze().cpu().numpy()\n",
    "        \n",
    "        advantages = self.compute_gae(rewards, values, dones)\n",
    "        advantages_tensor = torch.FloatTensor(advantages).to(self.device)\n",
    "        \n",
    "        # Store raw advantage statistics before normalization\n",
    "        raw_adv_mean = advantages_tensor.mean().item()\n",
    "        raw_adv_std = advantages_tensor.std().item()\n",
    "        raw_adv_max = advantages_tensor.max().item()\n",
    "        raw_adv_min = advantages_tensor.min().item()\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)\n",
    "        \n",
    "        returns = advantages_tensor + torch.FloatTensor(values).to(self.device)\n",
    "        \n",
    "        # Optimize policy for K epochs\n",
    "        total_grad_norm = 0.0\n",
    "        for _ in range(self.K_epochs):\n",
    "            logits, state_values = self.policy(states)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            action_logprobs = dist.log_prob(actions)\n",
    "            dist_entropy = dist.entropy()\n",
    "            \n",
    "            ratios = torch.exp(action_logprobs - old_logprobs)\n",
    "            \n",
    "            surr1 = ratios * advantages_tensor\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages_tensor\n",
    "            \n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = self.MseLoss(state_values.squeeze(), returns)\n",
    "            entropy_loss = -0.01 * dist_entropy.mean()\n",
    "            \n",
    "            loss = actor_loss + 0.5 * critic_loss + entropy_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "            total_grad_norm += grad_norm.item()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        avg_grad_norm = total_grad_norm / self.K_epochs\n",
    "        \n",
    "        # Calculate policy ratio statistics\n",
    "        with torch.no_grad():\n",
    "            final_logits, _ = self.policy(states)\n",
    "            final_dist = torch.distributions.Categorical(logits=final_logits)\n",
    "            final_logprobs = final_dist.log_prob(actions)\n",
    "            final_ratios = torch.exp(final_logprobs - old_logprobs)\n",
    "            \n",
    "            ratio_mean = final_ratios.mean().item()\n",
    "            ratio_std = final_ratios.std().item()\n",
    "            ratio_max = final_ratios.max().item()\n",
    "            ratio_min = final_ratios.min().item()\n",
    "        \n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        # Return comprehensive metrics\n",
    "        metrics = {\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'critic_loss': critic_loss.item(),\n",
    "            'entropy': -entropy_loss.item() / 0.01,  # Undo the scaling to get raw entropy\n",
    "            'grad_norm': avg_grad_norm,\n",
    "            'advantage_mean': raw_adv_mean,\n",
    "            'advantage_std': raw_adv_std,\n",
    "            'advantage_max': raw_adv_max,\n",
    "            'advantage_min': raw_adv_min,\n",
    "            'ratio_mean': ratio_mean,\n",
    "            'ratio_std': ratio_std,\n",
    "            'ratio_max': ratio_max,\n",
    "            'ratio_min': ratio_min,\n",
    "            'value_mean': np.mean(values),\n",
    "            'value_std': np.std(values),\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.policy.load_state_dict(torch.load(path))\n",
    "        self.policy_old.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name='Sokoban-v0', max_episodes=10000, max_timesteps=300, update_timestep=2048, save_freq=100):\n",
    "    import datetime\n",
    "    env = gym.make(env_name)\n",
    "    agent = PPOAgent(env)\n",
    "    \n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    \n",
    "    # Create log file with timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = f'logs/training_log_{timestamp}.txt'\n",
    "    \n",
    "    # Write header to log file\n",
    "    with open(log_file, 'w') as f:\n",
    "        f.write(\"=\" * 100 + \"\\n\")\n",
    "        f.write(f\"SOKOBAN PPO TRAINING LOG - Started at {datetime.datetime.now()}\\n\")\n",
    "        f.write(\"=\" * 100 + \"\\n\")\n",
    "        f.write(f\"Environment: {env_name}\\n\")\n",
    "        f.write(f\"Max Episodes: {max_episodes}\\n\")\n",
    "        f.write(f\"Max Timesteps per Episode: {max_timesteps}\\n\")\n",
    "        f.write(f\"Update Timestep: {update_timestep}\\n\")\n",
    "        f.write(f\"Save Frequency: {save_freq}\\n\")\n",
    "        f.write(\"=\" * 100 + \"\\n\\n\")\n",
    "        f.write(\"METRICS EXPLANATION:\\n\")\n",
    "        f.write(\"- Episode: Episode number\\n\")\n",
    "        f.write(\"- Reward: Total reward for this episode\\n\")\n",
    "        f.write(\"- Running Reward: Exponential moving average of rewards (smoothed trend)\\n\")\n",
    "        f.write(\"- Steps: Number of steps taken in this episode\\n\")\n",
    "        f.write(\"- Timestep: Total timesteps so far\\n\")\n",
    "        f.write(\"- Actor Loss: Policy improvement metric (more negative = more improvement)\\n\")\n",
    "        f.write(\"- Critic Loss: Value estimation error (lower = better predictions)\\n\")\n",
    "        f.write(\"- Entropy: Action randomness (higher = more exploration)\\n\")\n",
    "        f.write(\"- Grad Norm: Gradient magnitude (watch for explosion/vanishing)\\n\")\n",
    "        f.write(\"- Adv Mean/Std: Advantage statistics (measures action quality)\\n\")\n",
    "        f.write(\"- Ratio Mean: Policy change magnitude (should stay near 1.0)\\n\")\n",
    "        f.write(\"- Value Mean: Average predicted state value\\n\")\n",
    "        f.write(\"=\" * 100 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"Logging to: {log_file}\\n\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    running_reward = 0\n",
    "    timestep = 0\n",
    "    \n",
    "    # Track latest update metrics\n",
    "    latest_metrics = None\n",
    "    \n",
    "    memory = {\n",
    "        'states': [],\n",
    "        'actions': [],\n",
    "        'logprobs': [],\n",
    "        'rewards': [],\n",
    "        'dones': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(1, max_episodes + 1):\n",
    "        state = env.reset()\n",
    "        if len(state.shape) == 3:\n",
    "            state = np.transpose(state, (2, 0, 1))\n",
    "        \n",
    "        episode_reward = 0\n",
    "        \n",
    "        for t in range(max_timesteps):\n",
    "            timestep += 1\n",
    "            \n",
    "            action, action_logprob = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            if len(next_state.shape) == 3:\n",
    "                next_state = np.transpose(next_state, (2, 0, 1))\n",
    "            \n",
    "            memory['states'].append(state)\n",
    "            memory['actions'].append(action)\n",
    "            memory['logprobs'].append(action_logprob)\n",
    "            memory['rewards'].append(reward)\n",
    "            memory['dones'].append(done)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if timestep % update_timestep == 0:\n",
    "                latest_metrics = agent.update(memory)\n",
    "                memory = {\n",
    "                    'states': [],\n",
    "                    'actions': [],\n",
    "                    'logprobs': [],\n",
    "                    'rewards': [],\n",
    "                    'dones': []\n",
    "                }\n",
    "                print(f\"[UPDATE] Timestep {timestep} - Actor Loss: {latest_metrics['actor_loss']:.4f}, \"\n",
    "                      f\"Critic Loss: {latest_metrics['critic_loss']:.4f}, \"\n",
    "                      f\"Entropy: {latest_metrics['entropy']:.4f}\")\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_steps.append(t + 1)\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "        \n",
    "        # Console output\n",
    "        print(f\"Episode {episode:5d} | Reward: {episode_reward:7.2f} | \"\n",
    "              f\"Running: {running_reward:7.2f} | Steps: {t+1:3d}\")\n",
    "        \n",
    "        # Write to log file after EVERY episode\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"\\n{'='*100}\\n\")\n",
    "            f.write(f\"EPISODE {episode} (Timestep: {timestep})\\n\")\n",
    "            f.write(f\"{'='*100}\\n\")\n",
    "            f.write(f\"  Reward:         {episode_reward:10.4f}\\n\")\n",
    "            f.write(f\"  Running Reward: {running_reward:10.4f}\\n\")\n",
    "            f.write(f\"  Steps:          {t+1:10d}\\n\")\n",
    "            f.write(f\"  Total Timestep: {timestep:10d}\\n\")\n",
    "            \n",
    "            # Add update metrics if available (will be None for first few episodes)\n",
    "            if latest_metrics is not None:\n",
    "                f.write(f\"\\n  --- Latest Update Metrics (from timestep {(timestep // update_timestep) * update_timestep}) ---\\n\")\n",
    "                f.write(f\"  Actor Loss:      {latest_metrics['actor_loss']:10.6f}  (policy improvement)\\n\")\n",
    "                f.write(f\"  Critic Loss:     {latest_metrics['critic_loss']:10.6f}  (value prediction error)\\n\")\n",
    "                f.write(f\"  Entropy:         {latest_metrics['entropy']:10.6f}  (exploration level)\\n\")\n",
    "                f.write(f\"  Grad Norm:       {latest_metrics['grad_norm']:10.6f}  (gradient magnitude)\\n\")\n",
    "                f.write(f\"  \\n\")\n",
    "                f.write(f\"  Advantage Mean:  {latest_metrics['advantage_mean']:10.6f}\\n\")\n",
    "                f.write(f\"  Advantage Std:   {latest_metrics['advantage_std']:10.6f}\\n\")\n",
    "                f.write(f\"  Advantage Max:   {latest_metrics['advantage_max']:10.6f}\\n\")\n",
    "                f.write(f\"  Advantage Min:   {latest_metrics['advantage_min']:10.6f}\\n\")\n",
    "                f.write(f\"  \\n\")\n",
    "                f.write(f\"  Ratio Mean:      {latest_metrics['ratio_mean']:10.6f}  (policy change, should be ~1.0)\\n\")\n",
    "                f.write(f\"  Ratio Std:       {latest_metrics['ratio_std']:10.6f}\\n\")\n",
    "                f.write(f\"  Ratio Max:       {latest_metrics['ratio_max']:10.6f}\\n\")\n",
    "                f.write(f\"  Ratio Min:       {latest_metrics['ratio_min']:10.6f}\\n\")\n",
    "                f.write(f\"  \\n\")\n",
    "                f.write(f\"  Value Mean:      {latest_metrics['value_mean']:10.6f}  (avg predicted value)\\n\")\n",
    "                f.write(f\"  Value Std:       {latest_metrics['value_std']:10.6f}\\n\")\n",
    "        \n",
    "        # Save checkpoints\n",
    "        if episode % save_freq == 0:\n",
    "            agent.save(f'checkpoints/ppo_sokoban_ep{episode}.pth')\n",
    "            print(f\"[CHECKPOINT] Model saved at episode {episode}\")\n",
    "            \n",
    "            with open(log_file, 'a') as f:\n",
    "                f.write(f\"\\n  >>> CHECKPOINT SAVED: checkpoints/ppo_sokoban_ep{episode}.pth\\n\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Final summary\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"\\n\\n{'='*100}\\n\")\n",
    "        f.write(f\"TRAINING COMPLETED - {datetime.datetime.now()}\\n\")\n",
    "        f.write(f\"{'='*100}\\n\")\n",
    "        f.write(f\"Total Episodes:     {max_episodes}\\n\")\n",
    "        f.write(f\"Total Timesteps:    {timestep}\\n\")\n",
    "        f.write(f\"Final Running Reward: {running_reward:.4f}\\n\")\n",
    "        f.write(f\"Best Episode Reward:  {max(episode_rewards):.4f} (Episode {episode_rewards.index(max(episode_rewards)) + 1})\\n\")\n",
    "        f.write(f\"Worst Episode Reward: {min(episode_rewards):.4f} (Episode {episode_rewards.index(min(episode_rewards)) + 1})\\n\")\n",
    "        f.write(f\"Average Reward:       {np.mean(episode_rewards):.4f}\\n\")\n",
    "        f.write(f\"Average Steps:        {np.mean(episode_steps):.2f}\\n\")\n",
    "        f.write(f\"{'='*100}\\n\")\n",
    "    \n",
    "    print(f\"\\nTraining complete! Log saved to: {log_file}\")\n",
    "    \n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "Run the cell below to start training. You can adjust the parameters:\n",
    "- `max_episodes`: Total number of episodes to train\n",
    "- `max_timesteps`: Maximum steps per episode\n",
    "- `update_timestep`: How often to update the policy\n",
    "- `save_freq`: How often to save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\canid\\OneDrive\\Masaüstü\\Python\\CS_175\\sokobanRL\\.venv\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment Sokoban-v0 is out of date. You should consider upgrading to version `v2`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\canid\\OneDrive\\Masaüstü\\Python\\CS_175\\sokobanRL\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\canid\\OneDrive\\Masaüstü\\Python\\CS_175\\sokobanRL\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\canid\\OneDrive\\Masaüstü\\Python\\CS_175\\sokobanRL\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: logs/training_log_20251205_151025.txt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\canid\\OneDrive\\Masaüstü\\Python\\CS_175\\sokobanRL\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\Users\\canid\\OneDrive\\Masaüstü\\Python\\CS_175\\sokobanRL\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode     1 | Reward:  -20.00 | Running:   -1.00 | Steps: 200\n",
      "Episode     2 | Reward:  -20.00 | Running:   -1.95 | Steps: 200\n",
      "Episode     3 | Reward:  -20.00 | Running:   -2.85 | Steps: 200\n",
      "Episode     4 | Reward:  -20.00 | Running:   -3.71 | Steps: 200\n",
      "Episode     5 | Reward:  -20.00 | Running:   -4.52 | Steps: 200\n",
      "Episode     6 | Reward:  -20.00 | Running:   -5.30 | Steps: 200\n",
      "Episode     7 | Reward:  -20.00 | Running:   -6.03 | Steps: 200\n",
      "Episode     8 | Reward:  -20.00 | Running:   -6.73 | Steps: 200\n",
      "Episode     9 | Reward:  -19.00 | Running:   -7.35 | Steps: 200\n",
      "Episode    10 | Reward:  -20.00 | Running:   -7.98 | Steps: 200\n",
      "[UPDATE] Timestep 2048 - Actor Loss: 0.4347, Critic Loss: 96.3063, Entropy: 0.0000\n",
      "Episode    11 | Reward:  -20.00 | Running:   -8.58 | Steps: 200\n",
      "Episode    12 | Reward:  -20.00 | Running:   -9.15 | Steps: 200\n",
      "Episode    13 | Reward:  -20.00 | Running:   -9.69 | Steps: 200\n",
      "Episode    14 | Reward:  -20.00 | Running:  -10.21 | Steps: 200\n",
      "Episode    15 | Reward:  -20.00 | Running:  -10.70 | Steps: 200\n",
      "Episode    16 | Reward:  -20.00 | Running:  -11.16 | Steps: 200\n",
      "Episode    17 | Reward:  -20.00 | Running:  -11.60 | Steps: 200\n",
      "Episode    18 | Reward:  -20.00 | Running:  -12.02 | Steps: 200\n",
      "Episode    19 | Reward:  -20.00 | Running:  -12.42 | Steps: 200\n",
      "Episode    20 | Reward:  -20.00 | Running:  -12.80 | Steps: 200\n",
      "[UPDATE] Timestep 4096 - Actor Loss: 0.0000, Critic Loss: 3.3414, Entropy: 0.0000\n",
      "Episode    21 | Reward:  -20.00 | Running:  -13.16 | Steps: 200\n",
      "Episode    22 | Reward:  -20.00 | Running:  -13.50 | Steps: 200\n",
      "Episode    23 | Reward:  -20.00 | Running:  -13.83 | Steps: 200\n",
      "Episode    24 | Reward:  -20.00 | Running:  -14.14 | Steps: 200\n",
      "Episode    25 | Reward:  -20.00 | Running:  -14.43 | Steps: 200\n",
      "Episode    26 | Reward:  -20.00 | Running:  -14.71 | Steps: 200\n",
      "Episode    27 | Reward:  -20.00 | Running:  -14.97 | Steps: 200\n",
      "Episode    28 | Reward:  -20.00 | Running:  -15.22 | Steps: 200\n",
      "Episode    29 | Reward:  -20.00 | Running:  -15.46 | Steps: 200\n",
      "Episode    30 | Reward:  -20.00 | Running:  -15.69 | Steps: 200\n",
      "[UPDATE] Timestep 6144 - Actor Loss: -0.0000, Critic Loss: 1.6921, Entropy: 0.0011\n",
      "Episode    31 | Reward:  -20.00 | Running:  -15.91 | Steps: 200\n",
      "Episode    32 | Reward:  -20.00 | Running:  -16.11 | Steps: 200\n",
      "Episode    33 | Reward:  -20.00 | Running:  -16.30 | Steps: 200\n",
      "Episode    34 | Reward:  -20.00 | Running:  -16.49 | Steps: 200\n",
      "Episode    35 | Reward:  -20.00 | Running:  -16.67 | Steps: 200\n",
      "Episode    36 | Reward:  -20.00 | Running:  -16.83 | Steps: 200\n",
      "Episode    37 | Reward:  -20.00 | Running:  -16.99 | Steps: 200\n",
      "Episode    38 | Reward:  -20.00 | Running:  -17.14 | Steps: 200\n",
      "Episode    39 | Reward:  -20.00 | Running:  -17.28 | Steps: 200\n",
      "Episode    40 | Reward:  -20.00 | Running:  -17.42 | Steps: 200\n",
      "[UPDATE] Timestep 8192 - Actor Loss: 0.0001, Critic Loss: 2.7449, Entropy: 0.0719\n",
      "Episode    41 | Reward:  -20.00 | Running:  -17.55 | Steps: 200\n",
      "Episode    42 | Reward:  -20.00 | Running:  -17.67 | Steps: 200\n",
      "Episode    43 | Reward:  -20.00 | Running:  -17.79 | Steps: 200\n",
      "Episode    44 | Reward:  -20.00 | Running:  -17.90 | Steps: 200\n",
      "Episode    45 | Reward:  -20.00 | Running:  -18.00 | Steps: 200\n",
      "Episode    46 | Reward:  -20.00 | Running:  -18.10 | Steps: 200\n",
      "Episode    47 | Reward:  -20.00 | Running:  -18.20 | Steps: 200\n",
      "Episode    48 | Reward:  -20.00 | Running:  -18.29 | Steps: 200\n",
      "Episode    49 | Reward:  -20.00 | Running:  -18.37 | Steps: 200\n",
      "Episode    50 | Reward:  -20.00 | Running:  -18.45 | Steps: 200\n",
      "Episode    51 | Reward:  -20.00 | Running:  -18.53 | Steps: 200\n",
      "[UPDATE] Timestep 10240 - Actor Loss: 0.0131, Critic Loss: 2.5518, Entropy: 0.0512\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "episode_rewards = train(\n",
    "    env_name='Sokoban-v0',\n",
    "    max_episodes=10000,\n",
    "    max_timesteps=300,\n",
    "    update_timestep=2048,\n",
    "    save_freq=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training rewards\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Episode Rewards')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Plot moving average\n",
    "window_size = 100\n",
    "if len(episode_rewards) >= window_size:\n",
    "    moving_avg = np.convolve(episode_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(moving_avg)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title(f'Moving Average (window={window_size})')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(checkpoint_path, num_episodes=5, render=False):\n",
    "    \"\"\"Test a trained agent\"\"\"\n",
    "    env = gym.make('Sokoban-v0')\n",
    "    agent = PPOAgent(env)\n",
    "    agent.load(checkpoint_path)\n",
    "    \n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if len(state.shape) == 3:\n",
    "            state = np.transpose(state, (2, 0, 1))\n",
    "        \n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < 300:\n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            action, _ = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            if len(next_state.shape) == 3:\n",
    "                next_state = np.transpose(next_state, (2, 0, 1))\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Test Episode {episode + 1}: Reward = {episode_reward:.2f}, Steps = {steps}\")\n",
    "    \n",
    "    env.close()\n",
    "    print(f\"\\nAverage Reward: {np.mean(total_rewards):.2f}\")\n",
    "    return total_rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
