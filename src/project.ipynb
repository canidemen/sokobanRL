{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sokoban RL with PPO\n",
    "\n",
    "**CS 175 Final Project** - Training a PPO agent for Sokoban-small-v0\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. **Run cells 1-9** (setup and class definitions)\n",
    "2. **Run Cell 10 (DEMO)** - Quick demonstration with pre-trained agent\n",
    "3. Optionally view Cell 11 for comprehensive evaluation\n",
    "\n",
    "**Note:** Training from scratch (Cell 12) takes several hours.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Import Dependencies and Initialize Environment\n",
    "\n",
    "This cell imports all required libraries and sets up the Sokoban environment:\n",
    "- **gym & gym_sokoban**: Sokoban puzzle environment\n",
    "- **torch**: Deep learning framework for PPO agent\n",
    "- **matplotlib**: For visualization\n",
    "\n",
    "**CHECKPOINT_PATH**: Pre-trained model location (trained for 3000 episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "Compatibility patch applied ✓\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canid\\AppData\\Local\\Temp\\ipykernel_40604\\2034580231.py:15: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not hasattr(np, 'bool8'):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_sokoban\n",
    "import pygame\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "CHECKPOINT_PATH='google_colab_checkpoints/sokoban-small-v0/ppo_sokoban_ep3000.pth'\n",
    "\n",
    "# NumPy 2.x compatibility patch\n",
    "if not hasattr(np, 'bool8'):\n",
    "    np.bool8 = np.bool_\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"Compatibility patch applied ✓\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class SokobanRewardShaper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.previous_min_distance = None\n",
    "        self.boxes_on_target = 0\n",
    "        self.max_steps = 150\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        self.previous_min_distance = self._compute_min_box_target_distance()\n",
    "        self.boxes_on_target = self._count_boxes_on_target()\n",
    "        return obs\n",
    "\n",
    "    def _compute_min_box_target_distance(self):\n",
    "        room = self.env.unwrapped.room_state\n",
    "\n",
    "        boxes = np.argwhere((room == 3) | (room == 4))\n",
    "\n",
    "        targets = np.argwhere((room == 2) | (room == 3))\n",
    "\n",
    "        if len(boxes) == 0 or len(targets) == 0:\n",
    "            return 0\n",
    "\n",
    "        #manhattan distance\n",
    "        total_min_dist = 0\n",
    "        for box in boxes:\n",
    "            distances = np.abs(targets - box).sum(axis=1)\n",
    "            min_dist = distances.min()\n",
    "            total_min_dist += min_dist\n",
    "\n",
    "        return total_min_dist\n",
    "\n",
    "    def _count_boxes_on_target(self):\n",
    "        room = self.env.unwrapped.room_state\n",
    "        return np.sum(room == 3)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "\n",
    "        shaped_reward = reward\n",
    "\n",
    "        current_min_distance = self._compute_min_box_target_distance()\n",
    "        if self.previous_min_distance is not None:\n",
    "            distance_change = self.previous_min_distance - current_min_distance\n",
    "            if distance_change > 0:  # Moved closer\n",
    "                shaped_reward += 0.1 * distance_change\n",
    "            elif distance_change < 0:  # Moved away\n",
    "                shaped_reward += 0.1 * distance_change  # Small penalty\n",
    "\n",
    "        self.previous_min_distance = current_min_distance\n",
    "\n",
    "        current_boxes_on_target = self._count_boxes_on_target()\n",
    "        if current_boxes_on_target > self.boxes_on_target:\n",
    "            shaped_reward += 0.5 * (current_boxes_on_target - self.boxes_on_target)\n",
    "\n",
    "        self.boxes_on_target = current_boxes_on_target\n",
    "        \n",
    "        if done and not info.get('all_boxes_on_target', False):\n",
    "            if hasattr(self.env.unwrapped, 'num_env_steps'):\n",
    "                if self.env.unwrapped.num_env_steps >= self.max_steps - 1:\n",
    "                    shaped_reward -= 3.0  # Timeout penalty\n",
    "\n",
    "        return obs, shaped_reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Actor-Critic Neural Network Architecture\n",
    "\n",
    "CNN-based architecture for PPO agent:\n",
    "- **Actor head**: Outputs action probabilities (policy)\n",
    "- **Critic head**: Outputs state value estimates\n",
    "\n",
    "Features:\n",
    "- 3 convolutional layers with layer normalization\n",
    "- Orthogonal weight initialization for stable training\n",
    "- Separate heads for policy and value function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Convolutional layers with Layer Normalization for stability\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.ln1 = nn.LayerNorm([32, input_shape[1]//2, input_shape[2]//2])\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.ln2 = nn.LayerNorm([64, input_shape[1]//4, input_shape[2]//4])\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.ln3 = nn.LayerNorm([64, input_shape[1]//8, input_shape[2]//8])\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        # Actor head (policy) with smaller hidden layer\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 128),  # Reduced from 256\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "        \n",
    "        # Critic head (value function) with smaller hidden layer\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 128),  # Reduced from 256\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Orthogonal initialization for better gradient flow\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using orthogonal initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self._forward_conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def _forward_conv(self, x):\n",
    "        \"\"\"Forward pass through convolutional layers\"\"\"\n",
    "        x = torch.relu(self.ln1(self.conv1(x)))\n",
    "        x = torch.relu(self.ln2(self.conv2(x)))\n",
    "        x = torch.relu(self.ln3(self.conv3(x)))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self._forward_conv(x).view(x.size()[0], -1)\n",
    "        return self.actor(conv_out), self.critic(conv_out)\n",
    "    \n",
    "    def get_action_probs(self, x):\n",
    "        logits, _ = self.forward(x)\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        _, value = self.forward(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PPO (Proximal Policy Optimization) Agent\n",
    "\n",
    "Main reinforcement learning agent implementing PPO algorithm.\n",
    "\n",
    "**Key features:**\n",
    "- Generalized Advantage Estimation (GAE)\n",
    "- Clipped surrogate objective\n",
    "- Value function clipping\n",
    "- Learning rate warmup\n",
    "- Entropy regularization for exploration\n",
    "\n",
    "**Hyperparameters:** lr=3e-4, gamma=0.99, eps_clip=0.2, K_epochs=4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, env, lr=3e-4, gamma=0.99, eps_clip=0.2, K_epochs=4, gae_lambda=0.95, \n",
    "                 entropy_coef=0.05, value_clip=0.2, warmup_steps=10, advantage_clip=10.0):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_clip = value_clip\n",
    "        self.advantage_clip = advantage_clip\n",
    "        \n",
    "        # Learning rate warmup\n",
    "        self.base_lr = lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.current_update = 0\n",
    "        \n",
    "        # Get observation shape\n",
    "        obs = env.reset()\n",
    "        if len(obs.shape) == 3:\n",
    "            obs = np.transpose(obs, (2, 0, 1))\n",
    "        \n",
    "        self.input_shape = obs.shape\n",
    "        self.n_actions = env.action_space.n\n",
    "        \n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.policy = ActorCritic(self.input_shape, self.n_actions).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        self.policy_old = ActorCritic(self.input_shape, self.n_actions).to(self.device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "    \n",
    "    def _get_current_lr(self):\n",
    "        \"\"\"Get learning rate with warmup schedule\"\"\"\n",
    "        if self.current_update < self.warmup_steps:\n",
    "            # Linear warmup from 0 to base_lr\n",
    "            return self.base_lr * (self.current_update + 1) / self.warmup_steps\n",
    "        else:\n",
    "            return self.base_lr\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            action_probs = self.policy_old.get_action_probs(state)\n",
    "        \n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), action_logprob.item()\n",
    "    \n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = values[t + 1]\n",
    "            \n",
    "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        return advantages\n",
    "    \n",
    "    def update(self, memory):\n",
    "        self.current_update += 1\n",
    "        current_lr = self._get_current_lr()\n",
    "        \n",
    "        # Update learning rate\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = current_lr\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(memory['states'])).to(self.device)\n",
    "        actions = torch.LongTensor(memory['actions']).to(self.device)\n",
    "        old_logprobs = torch.FloatTensor(memory['logprobs']).to(self.device)\n",
    "        \n",
    "        rewards = memory['rewards']\n",
    "        dones = memory['dones']\n",
    "        \n",
    "        # Compute values and advantages\n",
    "        with torch.no_grad():\n",
    "            old_values = self.policy_old.get_value(states).squeeze().cpu().numpy()\n",
    "        \n",
    "        advantages = self.compute_gae(rewards, old_values, dones)\n",
    "        advantages_tensor = torch.FloatTensor(advantages).to(self.device)\n",
    "        \n",
    "        # Store raw advantage statistics before clipping/normalization\n",
    "        raw_adv_mean = advantages_tensor.mean().item()\n",
    "        raw_adv_std = advantages_tensor.std().item()\n",
    "        raw_adv_max = advantages_tensor.max().item()\n",
    "        raw_adv_min = advantages_tensor.min().item()\n",
    "        \n",
    "        # CLIP advantages to prevent extreme values\n",
    "        advantages_tensor = torch.clamp(advantages_tensor, -self.advantage_clip, self.advantage_clip)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)\n",
    "        \n",
    "        returns = advantages_tensor + torch.FloatTensor(old_values).to(self.device)\n",
    "        old_values_tensor = torch.FloatTensor(old_values).to(self.device)\n",
    "        \n",
    "        # Optimize policy for K epochs\n",
    "        total_grad_norm = 0.0\n",
    "        for _ in range(self.K_epochs):\n",
    "            logits, state_values = self.policy(states)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            action_logprobs = dist.log_prob(actions)\n",
    "            dist_entropy = dist.entropy()\n",
    "            \n",
    "            ratios = torch.exp(action_logprobs - old_logprobs)\n",
    "            \n",
    "            surr1 = ratios * advantages_tensor\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages_tensor\n",
    "            \n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Value function loss with clipping\n",
    "            state_values_squeeze = state_values.squeeze()\n",
    "            value_pred_clipped = old_values_tensor + torch.clamp(\n",
    "                state_values_squeeze - old_values_tensor,\n",
    "                -self.value_clip,\n",
    "                self.value_clip\n",
    "            )\n",
    "            value_loss1 = self.MseLoss(state_values_squeeze, returns)\n",
    "            value_loss2 = self.MseLoss(value_pred_clipped, returns)\n",
    "            critic_loss = torch.max(value_loss1, value_loss2)\n",
    "            \n",
    "            entropy_loss = -self.entropy_coef * dist_entropy.mean()\n",
    "            \n",
    "            loss = actor_loss + 0.5 * critic_loss + entropy_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "            total_grad_norm += grad_norm.item()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        avg_grad_norm = total_grad_norm / self.K_epochs\n",
    "        \n",
    "        # Calculate policy ratio statistics\n",
    "        with torch.no_grad():\n",
    "            final_logits, _ = self.policy(states)\n",
    "            final_dist = torch.distributions.Categorical(logits=final_logits)\n",
    "            final_logprobs = final_dist.log_prob(actions)\n",
    "            final_ratios = torch.exp(final_logprobs - old_logprobs)\n",
    "            \n",
    "            ratio_mean = final_ratios.mean().item()\n",
    "            ratio_std = final_ratios.std().item()\n",
    "            ratio_max = final_ratios.max().item()\n",
    "            ratio_min = final_ratios.min().item()\n",
    "        \n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        # Return comprehensive metrics\n",
    "        metrics = {\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'critic_loss': critic_loss.item(),\n",
    "            'entropy': -entropy_loss.item() / self.entropy_coef,\n",
    "            'grad_norm': avg_grad_norm,\n",
    "            'advantage_mean': raw_adv_mean,\n",
    "            'advantage_std': raw_adv_std,\n",
    "            'advantage_max': raw_adv_max,\n",
    "            'advantage_min': raw_adv_min,\n",
    "            'ratio_mean': ratio_mean,\n",
    "            'ratio_std': ratio_std,\n",
    "            'ratio_max': ratio_max,\n",
    "            'ratio_min': ratio_min,\n",
    "            'value_mean': np.mean(old_values),\n",
    "            'value_std': np.std(old_values),\n",
    "            'learning_rate': current_lr,\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.policy.load_state_dict(torch.load(path, weights_only=False))\n",
    "        self.policy_old.load_state_dict(torch.load(path, weights_only=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Function (For Training from Scratch)\n",
    "\n",
    "Training features:\n",
    "- Reward shaping for sparse reward environments\n",
    "- Checkpoint saving every 100 episodes\n",
    "- Detailed logging to logs/ directory\n",
    "- Resume training from checkpoint support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name='Sokoban-v0', max_episodes=10000, max_timesteps=300, update_timestep=2048, save_freq=100, \n",
    "          resume_from_checkpoint=None, start_episode=1):\n",
    "    import datetime\n",
    "    \n",
    "    # Enable reward shaping for sparse reward exploration\n",
    "    env = gym.make(env_name)\n",
    "    env = SokobanRewardShaper(env)  # Enable reward shaping\n",
    "    \n",
    "    agent = PPOAgent(env)\n",
    "    \n",
    "    # Load checkpoint if provided\n",
    "    if resume_from_checkpoint is not None:\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"RESUMING FROM CHECKPOINT: {resume_from_checkpoint}\")\n",
    "        print(f\"Starting from episode: {start_episode}\")\n",
    "        print(f\"{'='*100}\\n\")\n",
    "        agent.load(resume_from_checkpoint)\n",
    "    \n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    \n",
    "    # Create log file with timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if resume_from_checkpoint:\n",
    "        log_file = f'logs/training_log_resumed_{timestamp}.txt'\n",
    "    else:\n",
    "        log_file = f'logs/training_log_{timestamp}.txt'\n",
    "    \n",
    "    # Write header to log file\n",
    "    with open(log_file, 'w') as f:\n",
    "        f.write(\"=\" * 100 + \"\\n\")\n",
    "        if resume_from_checkpoint:\n",
    "            f.write(f\"SOKOBAN PPO TRAINING LOG (RESUMED) - Started at {datetime.datetime.now()}\\n\")\n",
    "            f.write(f\"Resumed from checkpoint: {resume_from_checkpoint}\\n\")\n",
    "            f.write(f\"Starting episode: {start_episode}\\n\")\n",
    "        else:\n",
    "            f.write(f\"SOKOBAN PPO TRAINING LOG (WITH REWARD SHAPING) - Started at {datetime.datetime.now()}\\n\")\n",
    "        f.write(\"=\" * 100 + \"\\n\")\n",
    "        f.write(f\"Environment: {env_name} (WITH REWARD SHAPING)\\n\")\n",
    "        f.write(f\"Max Episodes: {max_episodes}\\n\")\n",
    "        f.write(f\"Max Timesteps per Episode: {max_timesteps}\\n\")\n",
    "        f.write(f\"Update Timestep: {update_timestep}\\n\")\n",
    "        f.write(f\"Save Frequency: {save_freq}\\n\")\n",
    "        f.write(\"=\" * 100 + \"\\n\\n\")\n",
    "        f.write(\"HYPERPARAMETER IMPROVEMENTS + REWARD SHAPING:\\n\")\n",
    "        f.write(\"1. REWARD SHAPING ENABLED: Intermediate rewards for box movements\\n\")\n",
    "        f.write(\"   - +0.5 for each unit a box moves closer to targets\\n\")\n",
    "        f.write(\"   - +2.0 for placing a box on a target\\n\")\n",
    "        f.write(\"   - -0.2 for moving a box away from targets\\n\")\n",
    "        f.write(\"2. Layer Normalization: Stabilizes network activations\\n\")\n",
    "        f.write(\"3. Orthogonal Initialization: Better gradient flow\\n\")\n",
    "        f.write(\"4. Value Function Clipping: Prevents critic divergence\\n\")\n",
    "        f.write(\"5. Learning Rate Warmup: Gradual increase (10 updates)\\n\")\n",
    "        f.write(\"6. Advantage Clipping: Prevents extreme advantage values\\n\")\n",
    "        f.write(\"7. IMPROVED Hyperparameters:\\n\")\n",
    "        f.write(\"   - lr=3e-4 (standard PPO learning rate)\\n\")\n",
    "        f.write(\"   - entropy_coef=0.05 (exploration)\\n\")\n",
    "        f.write(\"   - grad_clip=0.5 (stable gradients)\\n\")\n",
    "        f.write(\"   - K_epochs=4\\n\")\n",
    "        f.write(\"   - warmup_steps=10\\n\")\n",
    "        f.write(\"=\" * 100 + \"\\n\\n\")\n",
    "        f.write(\"METRICS EXPLANATION:\\n\")\n",
    "        f.write(\"- Episode: Episode number\\n\")\n",
    "        f.write(\"- Reward: Total reward for this episode (WITH SHAPING BONUSES)\\n\")\n",
    "        f.write(\"- Running Reward: Exponential moving average of rewards\\n\")\n",
    "        f.write(\"- Steps: Number of steps taken in this episode\\n\")\n",
    "        f.write(\"- Timestep: Total timesteps so far\\n\")\n",
    "        f.write(\"- Actor Loss: Policy improvement metric\\n\")\n",
    "        f.write(\"- Critic Loss: Value estimation error\\n\")\n",
    "        f.write(\"- Entropy: Action randomness (target: 0.5-2.0)\\n\")\n",
    "        f.write(\"- Grad Norm: Gradient magnitude\\n\")\n",
    "        f.write(\"- Learning Rate: Current LR with warmup\\n\")\n",
    "        f.write(\"- Ratio Mean: Policy change (should stay near 1.0)\\n\")\n",
    "        f.write(\"=\" * 100 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"Logging to: {log_file}\\n\")\n",
    "    print(\"REWARD SHAPING ENABLED:\")\n",
    "    print(\"  - +0.5 per unit boxes move closer to targets\")\n",
    "    print(\"  - +2.0 for placing box on target\")\n",
    "    print(\"  - -0.2 per unit boxes move away from targets\")\n",
    "    print(\"\\nIMPROVED HYPERPARAMETERS ACTIVE:\")\n",
    "    print(\"  - Layer normalization + Orthogonal init\")\n",
    "    print(\"  - Value function clipping + Advantage clipping (±10)\")\n",
    "    print(\"  - Learning rate warmup (10 updates)\")\n",
    "    print(\"  - lr=3e-4, entropy=0.05, grad_clip=0.5, K_epochs=4\\n\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    running_reward = 0\n",
    "    timestep = 0\n",
    "    \n",
    "    # Track latest update metrics\n",
    "    latest_metrics = None\n",
    "    \n",
    "    memory = {\n",
    "        'states': [],\n",
    "        'actions': [],\n",
    "        'logprobs': [],\n",
    "        'rewards': [],\n",
    "        'dones': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(start_episode, max_episodes + 1):\n",
    "        state = env.reset()\n",
    "        if len(state.shape) == 3:\n",
    "            state = np.transpose(state, (2, 0, 1))\n",
    "        \n",
    "        episode_reward = 0\n",
    "        \n",
    "        for t in range(max_timesteps):\n",
    "            timestep += 1\n",
    "            \n",
    "            action, action_logprob = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            if len(next_state.shape) == 3:\n",
    "                next_state = np.transpose(next_state, (2, 0, 1))\n",
    "            \n",
    "            memory['states'].append(state)\n",
    "            memory['actions'].append(action)\n",
    "            memory['logprobs'].append(action_logprob)\n",
    "            memory['rewards'].append(reward)\n",
    "            memory['dones'].append(done)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if timestep % update_timestep == 0:\n",
    "                latest_metrics = agent.update(memory)\n",
    "                memory = {\n",
    "                    'states': [],\n",
    "                    'actions': [],\n",
    "                    'logprobs': [],\n",
    "                    'rewards': [],\n",
    "                    'dones': []\n",
    "                }\n",
    "                print(f\"[UPDATE {agent.current_update}] Timestep {timestep} - \"\n",
    "                      f\"LR: {latest_metrics['learning_rate']:.2e}, \"\n",
    "                      f\"Actor: {latest_metrics['actor_loss']:.4f}, \"\n",
    "                      f\"Critic: {latest_metrics['critic_loss']:.4f}, \"\n",
    "                      f\"Entropy: {latest_metrics['entropy']:.4f}, \"\n",
    "                      f\"GradNorm: {latest_metrics['grad_norm']:.4f}\")\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_steps.append(t + 1)\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "        \n",
    "        # Console output\n",
    "        print(f\"Episode {episode:5d} | Reward: {episode_reward:7.2f} | \"\n",
    "              f\"Running: {running_reward:7.2f} | Steps: {t+1:3d}\")\n",
    "        \n",
    "        # Write to log file after EVERY episode\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"\\n{'='*100}\\n\")\n",
    "            f.write(f\"EPISODE {episode} (Timestep: {timestep})\\n\")\n",
    "            f.write(f\"{'='*100}\\n\")\n",
    "            f.write(f\"  Reward:          {episode_reward:10.4f}\\n\")\n",
    "            f.write(f\"  Running Reward:  {running_reward:10.4f}\\n\")\n",
    "            f.write(f\"  Steps:           {t+1:10d}\\n\")\n",
    "            f.write(f\"  Total Timestep:  {timestep:10d}\\n\")\n",
    "            \n",
    "            # Add update metrics if available\n",
    "            if latest_metrics is not None:\n",
    "                f.write(f\"\\n  --- Latest Update Metrics (Update #{agent.current_update}) ---\\n\")\n",
    "                f.write(f\"  Learning Rate:   {latest_metrics['learning_rate']:10.8f}  (with warmup)\\n\")\n",
    "                f.write(f\"  Actor Loss:      {latest_metrics['actor_loss']:10.6f}\\n\")\n",
    "                f.write(f\"  Critic Loss:     {latest_metrics['critic_loss']:10.6f}\\n\")\n",
    "                f.write(f\"  Entropy:         {latest_metrics['entropy']:10.6f}\\n\")\n",
    "                f.write(f\"  Grad Norm:       {latest_metrics['grad_norm']:10.6f}\\n\")\n",
    "                f.write(f\"  \\n\")\n",
    "                f.write(f\"  Advantage Mean:  {latest_metrics['advantage_mean']:10.6f}\\n\")\n",
    "                f.write(f\"  Advantage Std:   {latest_metrics['advantage_std']:10.6f}\\n\")\n",
    "                f.write(f\"  Advantage Max:   {latest_metrics['advantage_max']:10.6f}\\n\")\n",
    "                f.write(f\"  Advantage Min:   {latest_metrics['advantage_min']:10.6f}\\n\")\n",
    "                f.write(f\"  \\n\")\n",
    "                f.write(f\"  Ratio Mean:      {latest_metrics['ratio_mean']:10.6f}\\n\")\n",
    "                f.write(f\"  Ratio Std:       {latest_metrics['ratio_std']:10.6f}\\n\")\n",
    "                f.write(f\"  Ratio Max:       {latest_metrics['ratio_max']:10.6f}\\n\")\n",
    "                f.write(f\"  Ratio Min:       {latest_metrics['ratio_min']:10.6f}\\n\")\n",
    "                f.write(f\"  \\n\")\n",
    "                f.write(f\"  Value Mean:      {latest_metrics['value_mean']:10.6f}\\n\")\n",
    "                f.write(f\"  Value Std:       {latest_metrics['value_std']:10.6f}\\n\")\n",
    "        \n",
    "        # Save checkpoints\n",
    "        if episode % save_freq == 0:\n",
    "            agent.save(f'checkpoints/ppo_sokoban_ep{episode}.pth')\n",
    "            print(f\"[CHECKPOINT] Model saved at episode {episode}\")\n",
    "            \n",
    "            with open(log_file, 'a') as f:\n",
    "                f.write(f\"\\n  >>> CHECKPOINT SAVED: checkpoints/ppo_sokoban_ep{episode}.pth\\n\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Final summary\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"\\n\\n{'='*100}\\n\")\n",
    "        f.write(f\"TRAINING COMPLETED - {datetime.datetime.now()}\\n\")\n",
    "        f.write(f\"{'='*100}\\n\")\n",
    "        f.write(f\"Total Episodes:       {max_episodes - start_episode + 1}\\n\")\n",
    "        f.write(f\"Total Timesteps:      {timestep}\\n\")\n",
    "        f.write(f\"Final Running Reward: {running_reward:.4f}\\n\")\n",
    "        if episode_rewards:\n",
    "            f.write(f\"Best Episode Reward:  {max(episode_rewards):.4f} (Episode {episode_rewards.index(max(episode_rewards)) + start_episode})\\n\")\n",
    "            f.write(f\"Worst Episode Reward: {min(episode_rewards):.4f} (Episode {episode_rewards.index(min(episode_rewards)) + start_episode})\\n\")\n",
    "            f.write(f\"Average Reward:       {np.mean(episode_rewards):.4f}\\n\")\n",
    "            f.write(f\"Average Steps:        {np.mean(episode_steps):.2f}\\n\")\n",
    "        f.write(f\"{'='*100}\\n\")\n",
    "    \n",
    "    print(f\"\\nTraining complete! Log saved to: {log_file}\")\n",
    "    \n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Resume Training from Checkpoint\n",
    "\n",
    "This cell will take hours to run.\n",
    "\n",
    "For quick evaluation, skip to **Section 7** instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment Sokoban-small-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "RESUMING FROM CHECKPOINT: google_colab_checkpoints/sokoban-small-v0/ppo_sokoban_ep3000.pth\n",
      "Starting from episode: 3001\n",
      "====================================================================================================\n",
      "\n",
      "Logging to: logs/training_log_resumed_20251212_144114.txt\n",
      "\n",
      "REWARD SHAPING ENABLED:\n",
      "  - +0.5 per unit boxes move closer to targets\n",
      "  - +2.0 for placing box on target\n",
      "  - -0.2 per unit boxes move away from targets\n",
      "\n",
      "IMPROVED HYPERPARAMETERS ACTIVE:\n",
      "  - Layer normalization + Orthogonal init\n",
      "  - Value function clipping + Advantage clipping (±10)\n",
      "  - Learning rate warmup (10 updates)\n",
      "  - lr=3e-4, entropy=0.05, grad_clip=0.5, K_epochs=4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  3001 | Reward:  -14.50 | Running:   -0.72 | Steps: 150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Resume training from ep3000 checkpoint on Sokoban-small-v0\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Remove optional path for training from scratch\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m episode_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSokoban-small-v0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Continue training until episode 10000\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Shorter episodes for smaller puzzles\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_timestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;66;43;03m# Save checkpoint every 100 episodes\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Path to uploaded checkpoint\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3001\u001b[39;49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Continue from episode 3001\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[43], line 113\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(env_name, max_episodes, max_timesteps, update_timestep, save_freq, resume_from_checkpoint, start_episode)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_timesteps):\n\u001b[0;32m    111\u001b[0m     timestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 113\u001b[0m     action, action_logprob \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     next_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(next_state\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "Cell \u001b[1;32mIn[42], line 46\u001b[0m, in \u001b[0;36mPPOAgent.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     44\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 46\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_old\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(action_probs)\n\u001b[0;32m     49\u001b[0m action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "Cell \u001b[1;32mIn[41], line 62\u001b[0m, in \u001b[0;36mActorCritic.get_action_probs\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_action_probs\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 62\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[41], line 59\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     58\u001b[0m     conv_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_conv(x)\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv_out\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(conv_out)\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03mRuns the forward pass.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Resume training from ep3000 checkpoint on Sokoban-small-v0\n",
    "# Remove optional path for training from scratch\n",
    "\n",
    "episode_rewards = train(\n",
    "    env_name='Sokoban-small-v0',\n",
    "    max_episodes=10000,              # Continue training until episode 10000\n",
    "    max_timesteps=150,               # Shorter episodes for smaller puzzles\n",
    "    update_timestep=2048,\n",
    "    save_freq=100,                   # Save checkpoint every 100 episodes\n",
    "    resume_from_checkpoint=CHECKPOINT_PATH,  # Path to uploaded checkpoint\n",
    "    start_episode=3001               # Continue from episode 3001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Functions\n",
    "\n",
    "Multiple evaluation options with different levels of detail:\n",
    "- **quick_evaluation**: 10 episodes, ~30 seconds (used in demo)\n",
    "- **test_agent**: 100 episodes, ~5 minutes\n",
    "- **comprehensive_evaluation**: 1000 episodes with graphs, ~30 minutes\n",
    "\n",
    "### Test Agent Function (100 Episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(checkpoint_path, env_name='Sokoban-small-v0', num_episodes=100, max_steps=200, render=False):\n",
    "    \"\"\"\n",
    "    Test a trained agent without reward shaping.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the checkpoint file\n",
    "        env_name: Environment to test on\n",
    "        num_episodes: Number of episodes to test\n",
    "        max_steps: Maximum steps per episode (default: 200)\n",
    "        render: Whether to render the environment\n",
    "    \n",
    "    Returns:\n",
    "        List of episode rewards (base Sokoban rewards without shaping)\n",
    "    \"\"\"\n",
    "    # Create environment WITHOUT reward shaping for true performance\n",
    "    env = gym.make(env_name)\n",
    "    agent = PPOAgent(env)\n",
    "    agent.load(checkpoint_path)\n",
    "    \n",
    "    total_rewards = []\n",
    "    success_count = 0\n",
    "    timeout_count = 0\n",
    "    \n",
    "    print(f\"Testing agent for {num_episodes} episodes on {env_name} (WITHOUT reward shaping)...\\n\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if len(state.shape) == 3:\n",
    "            state = np.transpose(state, (2, 0, 1))\n",
    "        \n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < max_steps:\n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            action, _ = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            if len(next_state.shape) == 3:\n",
    "                next_state = np.transpose(next_state, (2, 0, 1))\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        # Check success/failure status\n",
    "        success = done and info.get('all_boxes_on_target', False)\n",
    "        timeout = steps >= max_steps\n",
    "        \n",
    "        if success:\n",
    "            success_count += 1\n",
    "            status = \"✓ PASS\"\n",
    "        else:\n",
    "            status = \"✗ FAIL\"\n",
    "        \n",
    "        if timeout:\n",
    "            timeout_count += 1\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        \n",
    "        # Print every episode with pass/fail status\n",
    "        print(f\"Episode {episode + 1:3d}/{num_episodes} | Reward: {episode_reward:7.2f} | Steps: {steps:3d} | {status} | {'TIMEOUT' if timeout else ''}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    success_rate = (success_count / num_episodes) * 100\n",
    "    timeout_rate = (timeout_count / num_episodes) * 100\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL RESULTS:\")\n",
    "    print(f\"  Total Episodes:  {num_episodes}\")\n",
    "    print(f\"  Passed:          {success_count} ({success_rate:.1f}%)\")\n",
    "    print(f\"  Failed:          {num_episodes - success_count} ({100 - success_rate:.1f}%)\")\n",
    "    print(f\"  Timeouts:        {timeout_count} ({timeout_rate:.1f}%)\")\n",
    "    print(f\"  Average Reward:  {np.mean(total_rewards):.2f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Evaluation (1000 Episodes with Graphs)\n",
    "\n",
    "Takes a while to run (1000 episodes). For quick demo, use quick_evaluation instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive evaluation...\n",
      "This will take several minutes for 1000 episodes.\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE EVALUATION\n",
      "================================================================================\n",
      "Checkpoint:    google_colab_checkpoints/sokoban-small-v0/ppo_sokoban_ep3000.pth\n",
      "Environment:   Sokoban-small-v0\n",
      "Episodes:      1000\n",
      "Max Steps:     200\n",
      "================================================================================\n",
      "\n",
      "Episode    1/1000 | Reward:  -19.00 | Steps: 200 | ✗ FAIL  | TIMEOUT\n",
      "Episode    2/1000 | Reward:    4.60 | Steps:  74 | ✓ PASS  |        \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 394\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting comprehensive evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will take several minutes for 1000 episodes.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 394\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcomprehensive_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSokoban-small-v0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to False to hide individual episode output\u001b[39;49;00m\n\u001b[0;32m    400\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating presentation-ready graphs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[60], line 52\u001b[0m, in \u001b[0;36mcomprehensive_evaluation\u001b[1;34m(checkpoint_path, env_name, num_episodes, max_steps, verbose)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m steps \u001b[38;5;241m<\u001b[39m max_steps:\n\u001b[0;32m     51\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[1;32m---> 52\u001b[0m     next_state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(next_state\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m     55\u001b[0m         next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(next_state, (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym_sokoban\\envs\\sokoban_env.py:80\u001b[0m, in \u001b[0;36mSokobanEnv.step\u001b[1;34m(self, action, observation_mode)\u001b[0m\n\u001b[0;32m     77\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_done()\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Convert the observation to RGB frame\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m info \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction.name\u001b[39m\u001b[38;5;124m\"\u001b[39m: ACTION_LOOKUP[action],\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction.moved_player\u001b[39m\u001b[38;5;124m\"\u001b[39m: moved_player,\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction.moved_box\u001b[39m\u001b[38;5;124m\"\u001b[39m: moved_box,\n\u001b[0;32m     86\u001b[0m }\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym_sokoban\\envs\\sokoban_env.py:225\u001b[0m, in \u001b[0;36mSokobanEnv.render\u001b[1;34m(self, mode, close, scale)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m, close\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m RENDERING_MODES\n\u001b[1;32m--> 225\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym_sokoban\\envs\\sokoban_env.py:245\u001b[0m, in \u001b[0;36mSokobanEnv.get_image\u001b[1;34m(self, mode, scale)\u001b[0m\n\u001b[0;32m    243\u001b[0m     img \u001b[38;5;241m=\u001b[39m room_to_tiny_world_rgb(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroom_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroom_fixed, scale\u001b[38;5;241m=\u001b[39mscale)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 245\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mroom_to_rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroom_fixed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym_sokoban\\envs\\render_utils.py:22\u001b[0m, in \u001b[0;36mroom_to_rgb\u001b[1;34m(room, room_structure)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Load images, representing the corresponding situation\u001b[39;00m\n\u001b[0;32m     21\u001b[0m box_filename \u001b[38;5;241m=\u001b[39m pkg_resources\u001b[38;5;241m.\u001b[39mresource_filename(resource_package, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurface\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbox.png\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m---> 22\u001b[0m box \u001b[38;5;241m=\u001b[39m \u001b[43mimageio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m box_on_target_filename \u001b[38;5;241m=\u001b[39m pkg_resources\u001b[38;5;241m.\u001b[39mresource_filename(resource_package,\n\u001b[0;32m     25\u001b[0m                                                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurface\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbox_on_target.png\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m     26\u001b[0m box_on_target \u001b[38;5;241m=\u001b[39m imageio\u001b[38;5;241m.\u001b[39mimread(box_on_target_filename)\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\imageio\\__init__.py:100\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(uri, format, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"imread(uri, format=None, **kwargs)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mReads an image from the specified file. Returns a numpy array, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m    to see what arguments are available for a particular format.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting with ImageIO v3 the behavior of this function will switch to that of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m iio.v3.imread. To keep the current behavior (and make this warning disappear)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     98\u001b[0m )\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m imread_v2(uri, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\imageio\\v2.py:359\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(uri, format, **kwargs)\u001b[0m\n\u001b[0;32m    356\u001b[0m imopen_args \u001b[38;5;241m=\u001b[39m decypher_format_arg(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m    357\u001b[0m imopen_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mri\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimopen_args) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m    360\u001b[0m     result \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\imageio\\core\\imopen.py:113\u001b[0m, in \u001b[0;36mimopen\u001b[1;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m     request\u001b[38;5;241m.\u001b[39mformat_hint \u001b[38;5;241m=\u001b[39m format_hint\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mio_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<bytes>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(uri, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m uri\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# fast-path based on plugin\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# (except in legacy mode)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\imageio\\core\\request.py:250\u001b[0m, in \u001b[0;36mRequest.__init__\u001b[1;34m(self, uri, mode, extension, format_hint, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Request.Mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# Parse what was given\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# Set extension\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extension \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\imageio\\core\\request.py:384\u001b[0m, in \u001b[0;36mRequest._parse_uri\u001b[1;34m(self, uri)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename_zip:\n\u001b[0;32m    383\u001b[0m         fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename_zip[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;129;01mand\u001b[39;00m (fn \u001b[38;5;129;01min\u001b[39;00m EXAMPLE_IMAGES):\n\u001b[0;32m    385\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    386\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m. This file looks like one of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe standard images, but from imageio 2.1, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    388\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandard images have to be specified using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimageio:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (fn, fn)\n\u001b[0;32m    390\u001b[0m         )\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Make filename absolute\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def comprehensive_evaluation(checkpoint_path, env_name='Sokoban-small-v0', num_episodes=1000, max_steps=200, verbose=True):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation with detailed metrics and timing.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "        env_name: Environment name\n",
    "        num_episodes: Number of test episodes\n",
    "        max_steps: Max steps per episode\n",
    "        verbose: If True, print each episode result\n",
    "    \"\"\"\n",
    "    # Create environment WITHOUT reward shaping for true performance\n",
    "    env = gym.make(env_name)\n",
    "    agent = PPOAgent(env)\n",
    "    agent.load(checkpoint_path)\n",
    "    \n",
    "    # Metrics storage\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    episode_times = []\n",
    "    success_flags = []\n",
    "    timeout_flags = []\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"COMPREHENSIVE EVALUATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Checkpoint:    {checkpoint_path}\")\n",
    "    print(f\"Environment:   {env_name}\")\n",
    "    print(f\"Episodes:      {num_episodes}\")\n",
    "    print(f\"Max Steps:     {max_steps}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    start_total = time.time()\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if len(state.shape) == 3:\n",
    "            state = np.transpose(state, (2, 0, 1))\n",
    "        \n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        episode_start = time.time()\n",
    "        \n",
    "        while not done and steps < max_steps:\n",
    "            action, _ = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            if len(next_state.shape) == 3:\n",
    "                next_state = np.transpose(next_state, (2, 0, 1))\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        episode_time = time.time() - episode_start\n",
    "        \n",
    "        # Check success/failure\n",
    "        success = done and info.get('all_boxes_on_target', False)\n",
    "        timeout = steps >= max_steps\n",
    "        \n",
    "        # Record metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_steps.append(steps)\n",
    "        episode_times.append(episode_time)\n",
    "        success_flags.append(1 if success else 0)\n",
    "        timeout_flags.append(1 if timeout else 0)\n",
    "        \n",
    "        # Print individual episode result if verbose\n",
    "        if verbose:\n",
    "            status = \"✓ PASS\" if success else \"✗ FAIL\"\n",
    "            timeout_marker = \"TIMEOUT\" if timeout else \"\"\n",
    "            print(f\"Episode {episode + 1:4d}/{num_episodes} | \"\n",
    "                  f\"Reward: {episode_reward:7.2f} | \"\n",
    "                  f\"Steps: {steps:3d} | \"\n",
    "                  f\"{status:7s} | \"\n",
    "                  f\"{timeout_marker:7s}\")\n",
    "        \n",
    "        # Progress summary every 100 episodes (even if not verbose)\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            current_success_rate = sum(success_flags) / len(success_flags) * 100\n",
    "            avg_reward = np.mean(episode_rewards)\n",
    "            avg_steps = np.mean(episode_steps)\n",
    "            avg_time = np.mean(episode_times)\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"PROGRESS UPDATE - Episode {episode + 1}/{num_episodes}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"  Success Rate:       {current_success_rate:6.2f}%\")\n",
    "            print(f\"  Avg Reward:         {avg_reward:7.2f}\")\n",
    "            print(f\"  Avg Steps:          {avg_steps:6.1f}\")\n",
    "            print(f\"  Avg Episode Time:   {avg_time:7.4f}s\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    total_time = time.time() - start_total\n",
    "    env.close()\n",
    "    \n",
    "    # Calculate comprehensive statistics\n",
    "    success_rate = sum(success_flags) / num_episodes * 100\n",
    "    timeout_rate = sum(timeout_flags) / num_episodes * 100\n",
    "    \n",
    "    success_episodes = [i for i in range(num_episodes) if success_flags[i] == 1]\n",
    "    failure_episodes = [i for i in range(num_episodes) if success_flags[i] == 0]\n",
    "    \n",
    "    success_rewards = [episode_rewards[i] for i in success_episodes]\n",
    "    failure_rewards = [episode_rewards[i] for i in failure_episodes]\n",
    "    \n",
    "    success_steps_list = [episode_steps[i] for i in success_episodes]\n",
    "    failure_steps_list = [episode_steps[i] for i in failure_episodes]\n",
    "    \n",
    "    # Print comprehensive data overview\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL EVALUATION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n{'OVERALL PERFORMANCE':^80}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Total Episodes:          {num_episodes}\")\n",
    "    print(f\"  Successful Episodes:     {len(success_episodes)} ({success_rate:.2f}%)\")\n",
    "    print(f\"  Failed Episodes:         {len(failure_episodes)} ({100-success_rate:.2f}%)\")\n",
    "    print(f\"  Timeout Episodes:        {sum(timeout_flags)} ({timeout_rate:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'REWARD STATISTICS':^80}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Overall:\")\n",
    "    print(f\"    Mean:                  {np.mean(episode_rewards):7.2f}\")\n",
    "    print(f\"    Median:                {np.median(episode_rewards):7.2f}\")\n",
    "    print(f\"    Std Dev:               {np.std(episode_rewards):7.2f}\")\n",
    "    print(f\"    Min:                   {min(episode_rewards):7.2f}\")\n",
    "    print(f\"    Max:                   {max(episode_rewards):7.2f}\")\n",
    "    \n",
    "    if success_rewards:\n",
    "        print(f\"  Success Episodes:\")\n",
    "        print(f\"    Mean:                  {np.mean(success_rewards):7.2f}\")\n",
    "        print(f\"    Median:                {np.median(success_rewards):7.2f}\")\n",
    "        print(f\"    Std Dev:               {np.std(success_rewards):7.2f}\")\n",
    "    \n",
    "    if failure_rewards:\n",
    "        print(f\"  Failure Episodes:\")\n",
    "        print(f\"    Mean:                  {np.mean(failure_rewards):7.2f}\")\n",
    "        print(f\"    Median:                {np.median(failure_rewards):7.2f}\")\n",
    "        print(f\"    Std Dev:               {np.std(failure_rewards):7.2f}\")\n",
    "    \n",
    "    print(f\"\\n{'EPISODE DURATION (STEPS)':^80}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Overall:\")\n",
    "    print(f\"    Mean:                  {np.mean(episode_steps):7.1f}\")\n",
    "    print(f\"    Median:                {np.median(episode_steps):7.1f}\")\n",
    "    print(f\"    Min:                   {min(episode_steps):7d}\")\n",
    "    print(f\"    Max:                   {max(episode_steps):7d}\")\n",
    "    \n",
    "    if success_steps_list:\n",
    "        print(f\"  Success Episodes:\")\n",
    "        print(f\"    Mean:                  {np.mean(success_steps_list):7.1f}\")\n",
    "        print(f\"    Median:                {np.median(success_steps_list):7.1f}\")\n",
    "        print(f\"    Min:                   {min(success_steps_list):7d}\")\n",
    "        print(f\"    Max:                   {max(success_steps_list):7d}\")\n",
    "    \n",
    "    if failure_steps_list:\n",
    "        print(f\"  Failure Episodes:\")\n",
    "        print(f\"    Mean:                  {np.mean(failure_steps_list):7.1f}\")\n",
    "        print(f\"    Median:                {np.median(failure_steps_list):7.1f}\")\n",
    "    \n",
    "    print(f\"\\n{'TIMING STATISTICS':^80}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Avg Episode Time:        {np.mean(episode_times):7.4f}s\")\n",
    "    print(f\"  Total Evaluation Time:   {total_time:7.2f}s\")\n",
    "    print(f\"  Episodes per Second:     {num_episodes/total_time:7.2f}\")\n",
    "    \n",
    "    print(f\"\\n{'QUARTILE ANALYSIS':^80}\")\n",
    "    print(\"-\" * 80)\n",
    "    q1, q2, q3 = np.percentile(episode_rewards, [25, 50, 75])\n",
    "    print(f\"  25th Percentile (Q1):    {q1:7.2f}\")\n",
    "    print(f\"  50th Percentile (Q2):    {q2:7.2f}\")\n",
    "    print(f\"  75th Percentile (Q3):    {q3:7.2f}\")\n",
    "    print(f\"  IQR (Q3 - Q1):           {q3-q1:7.2f}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'steps': episode_steps,\n",
    "        'times': episode_times,\n",
    "        'success': success_flags,\n",
    "        'timeout': timeout_flags,\n",
    "        'success_rate': success_rate,\n",
    "        'total_time': total_time\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_individual_graphs(results, checkpoint_name='ep3000'):\n",
    "    \"\"\"\n",
    "    Create 5 separate, presentation-quality graphs for Google Slides.\n",
    "    Each graph is displayed individually for easy copying.\n",
    "    \"\"\"\n",
    "    rewards = results['rewards']\n",
    "    steps = results['steps']\n",
    "    success = results['success']\n",
    "    times = results['times']\n",
    "    \n",
    "    window = 50\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING PRESENTATION GRAPHS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Each graph will be displayed separately for easy copying to slides.\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # ========== GRAPH 1: Average Reward & Success Rate vs Episodes (NEW!) ==========\n",
    "    print(\"Graph 1/5: Average Reward & Success Rate vs Episodes\")\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # Calculate rolling averages\n",
    "    rolling_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    rolling_success = np.convolve(success, np.ones(window)/window, mode='valid') * 100\n",
    "    \n",
    "    # Plot reward curve\n",
    "    color1 = 'tab:blue'\n",
    "    ax1.set_xlabel('Episode', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Average Reward (50-ep window)', color=color1, fontsize=14, fontweight='bold')\n",
    "    line1 = ax1.plot(range(window-1, len(rewards)), rolling_rewards, color=color1, \n",
    "                     linewidth=2.5, label='Avg Reward', alpha=0.9)\n",
    "    ax1.tick_params(axis='y', labelcolor=color1, labelsize=12)\n",
    "    ax1.grid(True, alpha=0.4, linestyle='--')\n",
    "    ax1.set_xlim([0, len(rewards)])\n",
    "    \n",
    "    # Plot success rate on secondary axis\n",
    "    ax2 = ax1.twinx()\n",
    "    color2 = 'tab:orange'\n",
    "    ax2.set_ylabel('Success Rate % (50-ep window)', color=color2, fontsize=14, fontweight='bold')\n",
    "    line2 = ax2.plot(range(window-1, len(success)), rolling_success, color=color2, \n",
    "                     linewidth=2.5, linestyle='--', label='Success Rate', alpha=0.9)\n",
    "    ax2.tick_params(axis='y', labelcolor=color2, labelsize=12)\n",
    "    ax2.set_xlim([0, len(success)])\n",
    "    \n",
    "    # Combined legend\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, loc='upper left', fontsize=12, framealpha=0.9)\n",
    "    \n",
    "    plt.title(f'Agent Performance: Reward & Success Rate - {checkpoint_name}', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print()\n",
    "    \n",
    "    # ========== GRAPH 2: Average Reward Curve (50-ep Rolling Window) ==========\n",
    "    print(\"Graph 2/5: Average Reward Curve\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    rolling_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    ax.plot(range(window-1, len(rewards)), rolling_rewards, color='tab:blue', \n",
    "            linewidth=2.5, label=f'{window}-Episode Moving Average')\n",
    "    ax.axhline(y=np.mean(rewards), color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Overall Mean: {np.mean(rewards):.2f}', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Episode', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Average Reward', fontsize=14, fontweight='bold')\n",
    "    ax.set_title(f'Average Reward Over Time - {checkpoint_name}', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.legend(fontsize=12, framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.4, linestyle='--')\n",
    "    ax.tick_params(labelsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print()\n",
    "    \n",
    "    # ========== GRAPH 3: Cumulative Success Rate ==========\n",
    "    print(\"Graph 3/5: Cumulative Success Rate\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    cumulative_success = np.cumsum(success) / np.arange(1, len(success) + 1) * 100\n",
    "    \n",
    "    ax.plot(cumulative_success, color='green', linewidth=3, alpha=0.9)\n",
    "    ax.axhline(y=results['success_rate'], color='red', linestyle='--', linewidth=2.5, \n",
    "               label=f\"Final Success Rate: {results['success_rate']:.1f}%\", alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Episode', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Cumulative Success Rate (%)', fontsize=14, fontweight='bold')\n",
    "    ax.set_title(f'Cumulative Success Rate - {checkpoint_name}', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.grid(True, alpha=0.4, linestyle='--')\n",
    "    ax.legend(fontsize=12, framealpha=0.9)\n",
    "    ax.set_ylim([0, max(cumulative_success) * 1.1])\n",
    "    ax.tick_params(labelsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print()\n",
    "    \n",
    "    # ========== GRAPH 4: Average Episode Duration (Steps) ==========\n",
    "    print(\"Graph 4/5: Average Episode Duration\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    rolling_steps = np.convolve(steps, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    ax.plot(range(window-1, len(steps)), rolling_steps, color='purple', \n",
    "            linewidth=2.5, label=f'{window}-Episode Moving Average')\n",
    "    ax.axhline(y=np.mean(steps), color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Overall Mean: {np.mean(steps):.1f} steps', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Episode', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Steps per Episode', fontsize=14, fontweight='bold')\n",
    "    ax.set_title(f'Average Episode Duration - {checkpoint_name}', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.grid(True, alpha=0.4, linestyle='--')\n",
    "    ax.legend(fontsize=12, framealpha=0.9)\n",
    "    ax.tick_params(labelsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print()\n",
    "    \n",
    "    # ========== GRAPH 5: Performance Metrics Dashboard ==========\n",
    "    print(\"Graph 5/5: Performance Metrics Dashboard\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create comprehensive metrics text\n",
    "    metrics_text = f\"\"\"\n",
    "    PERFORMANCE METRICS SUMMARY\n",
    "    {'=' * 50}\n",
    "    \n",
    "    Checkpoint:             {checkpoint_name}\n",
    "    Episodes Tested:        {len(rewards)}\n",
    "    \n",
    "    {'SUCCESS METRICS':^50}\n",
    "    {'-' * 50}\n",
    "    Success Rate:           {results['success_rate']:.2f}%\n",
    "    Timeout Rate:           {sum(results['timeout'])/len(results['timeout'])*100:.2f}%\n",
    "    \n",
    "    {'REWARD STATISTICS':^50}\n",
    "    {'-' * 50}\n",
    "    Mean Reward:            {np.mean(rewards):7.2f}\n",
    "    Median Reward:          {np.median(rewards):7.2f}\n",
    "    Std Deviation:          {np.std(rewards):7.2f}\n",
    "    Min Reward:             {min(rewards):7.2f}\n",
    "    Max Reward:             {max(rewards):7.2f}\n",
    "    \n",
    "    {'EPISODE DURATION':^50}\n",
    "    {'-' * 50}\n",
    "    Mean Steps:             {np.mean(steps):7.1f}\n",
    "    Median Steps:           {np.median(steps):7.1f}\n",
    "    Min Steps:              {min(steps):7d}\n",
    "    Max Steps:              {max(steps):7d}\n",
    "    \n",
    "    {'TIMING ANALYSIS':^50}\n",
    "    {'-' * 50}\n",
    "    Avg Episode Time:       {np.mean(times):7.4f}s\n",
    "    Total Eval Time:        {results['total_time']:7.2f}s\n",
    "    Episodes/Second:        {len(rewards)/results['total_time']:7.2f}\n",
    "    \n",
    "    {'QUARTILE ANALYSIS':^50}\n",
    "    {'-' * 50}\n",
    "    Q1 (25th percentile):   {np.percentile(rewards, 25):7.2f}\n",
    "    Q2 (50th percentile):   {np.percentile(rewards, 50):7.2f}\n",
    "    Q3 (75th percentile):   {np.percentile(rewards, 75):7.2f}\n",
    "    IQR (Q3 - Q1):          {np.percentile(rewards, 75) - np.percentile(rewards, 25):7.2f}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax.text(0.05, 0.95, metrics_text, transform=ax.transAxes,\n",
    "            fontsize=13, verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8, pad=1.5))\n",
    "    \n",
    "    plt.title(f'Evaluation Summary - {checkpoint_name}', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ALL GRAPHS GENERATED SUCCESSFULLY!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nYou can now right-click each graph above to copy/save individually.\")\n",
    "    print(\"Perfect for adding to your Google Slides presentation!\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "print(\"Starting comprehensive evaluation...\")\n",
    "print(\"This will take several minutes for 1000 episodes.\\n\")\n",
    "\n",
    "results = comprehensive_evaluation(\n",
    "    CHECKPOINT_PATH, \n",
    "    env_name='Sokoban-small-v0', \n",
    "    num_episodes=1000, \n",
    "    max_steps=200,\n",
    "    verbose=True  # Set to False to hide individual episode output\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Generating presentation-ready graphs...\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "plot_individual_graphs(results, checkpoint_name='ep3000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Evaluation Function (10 Episodes)\n",
    "\n",
    "Lightweight evaluation for quick testing:\n",
    "- Runs 10 episodes (~30 seconds)\n",
    "- No graphs (print output only)\n",
    "- Shows success rate and performance metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def quick_evaluation(checkpoint_path, env_name='Sokoban-small-v0', num_episodes=10, max_steps=200, verbose=True):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation with detailed metrics and timing.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "        env_name: Environment name\n",
    "        num_episodes: Number of test episodes\n",
    "        max_steps: Max steps per episode\n",
    "        verbose: If True, print each episode result\n",
    "    \"\"\"\n",
    "    # Create environment WITHOUT reward shaping for true performance\n",
    "    env = gym.make(env_name)\n",
    "    agent = PPOAgent(env)\n",
    "    agent.load(checkpoint_path)\n",
    "    \n",
    "    # Metrics storage\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    episode_times = []\n",
    "    success_flags = []\n",
    "    timeout_flags = []\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"QUICK EVALUATION (10 Episodes)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Checkpoint:    {checkpoint_path}\")\n",
    "    print(f\"Environment:   {env_name}\")\n",
    "    print(f\"Episodes:      {num_episodes}\")\n",
    "    print(f\"Max Steps:     {max_steps}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    start_total = time.time()\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if len(state.shape) == 3:\n",
    "            state = np.transpose(state, (2, 0, 1))\n",
    "        \n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        episode_start = time.time()\n",
    "        \n",
    "        while not done and steps < max_steps:\n",
    "            action, _ = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            if len(next_state.shape) == 3:\n",
    "                next_state = np.transpose(next_state, (2, 0, 1))\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        episode_time = time.time() - episode_start\n",
    "        \n",
    "        # Check success/failure\n",
    "        success = done and info.get('all_boxes_on_target', False)\n",
    "        timeout = steps >= max_steps\n",
    "        \n",
    "        # Record metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_steps.append(steps)\n",
    "        episode_times.append(episode_time)\n",
    "        success_flags.append(1 if success else 0)\n",
    "        timeout_flags.append(1 if timeout else 0)\n",
    "        \n",
    "        # Print individual episode result if verbose\n",
    "        if verbose:\n",
    "            status = \"✓ PASS\" if success else \"✗ FAIL\"\n",
    "            timeout_marker = \"TIMEOUT\" if timeout else \"\"\n",
    "            print(f\"Episode {episode + 1:4d}/{num_episodes} | \"\n",
    "                  f\"Reward: {episode_reward:7.2f} | \"\n",
    "                  f\"Steps: {steps:3d} | \"\n",
    "                  f\"{status:7s} | \"\n",
    "                  f\"{timeout_marker:7s}\")\n",
    "        \n",
    "        # Progress summary every 100 episodes (even if not verbose)\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            current_success_rate = sum(success_flags) / len(success_flags) * 100\n",
    "            avg_reward = np.mean(episode_rewards)\n",
    "            avg_steps = np.mean(episode_steps)\n",
    "            avg_time = np.mean(episode_times)\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"PROGRESS UPDATE - Episode {episode + 1}/{num_episodes}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"  Success Rate:       {current_success_rate:6.2f}%\")\n",
    "            print(f\"  Avg Reward:         {avg_reward:7.2f}\")\n",
    "            print(f\"  Avg Steps:          {avg_steps:6.1f}\")\n",
    "            print(f\"  Avg Episode Time:   {avg_time:7.4f}s\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    total_time = time.time() - start_total\n",
    "    env.close()\n",
    "    \n",
    "    # Calculate comprehensive statistics\n",
    "    success_rate = sum(success_flags) / num_episodes * 100\n",
    "    timeout_rate = sum(timeout_flags) / num_episodes * 100\n",
    "    \n",
    "    success_episodes = [i for i in range(num_episodes) if success_flags[i] == 1]\n",
    "    failure_episodes = [i for i in range(num_episodes) if success_flags[i] == 0]\n",
    "    \n",
    "    success_rewards = [episode_rewards[i] for i in success_episodes]\n",
    "    failure_rewards = [episode_rewards[i] for i in failure_episodes]\n",
    "    \n",
    "    success_steps_list = [episode_steps[i] for i in success_episodes]\n",
    "    failure_steps_list = [episode_steps[i] for i in failure_episodes]\n",
    "    \n",
    "    # Print comprehensive data overview\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL EVALUATION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n{'OVERALL PERFORMANCE':^80}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Total Episodes:          {num_episodes}\")\n",
    "    print(f\"  Successful Episodes:     {len(success_episodes)} ({success_rate:.2f}%)\")\n",
    "    print(f\"  Failed Episodes:         {len(failure_episodes)} ({100-success_rate:.2f}%)\")\n",
    "    print(f\"  Timeout Episodes:        {sum(timeout_flags)} ({timeout_rate:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'REWARD STATISTICS':^80}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Overall:\")\n",
    "    print(f\"    Mean:                  {np.mean(episode_rewards):7.2f}\")\n",
    "    print(f\"    Median:                {np.median(episode_rewards):7.2f}\")\n",
    "    print(f\"    Std Dev:               {np.std(episode_rewards):7.2f}\")\n",
    "    print(f\"    Min:                   {min(episode_rewards):7.2f}\")\n",
    "    print(f\"    Max:                   {max(episode_rewards):7.2f}\")\n",
    "    \n",
    "    if success_rewards:\n",
    "        print(f\"  Success Episodes:\")\n",
    "        print(f\"    Mean:                  {np.mean(success_rewards):7.2f}\")\n",
    "        print(f\"    Median:                {np.median(success_rewards):7.2f}\")\n",
    "        print(f\"    Std Dev:               {np.std(success_rewards):7.2f}\")\n",
    "    \n",
    "    if failure_rewards:\n",
    "        print(f\"  Failure Episodes:\")\n",
    "        print(f\"    Mean:                  {np.mean(failure_rewards):7.2f}\")\n",
    "        print(f\"    Median:                {np.median(failure_rewards):7.2f}\")\n",
    "        print(f\"    Std Dev:               {np.std(failure_rewards):7.2f}\")\n",
    "    \n",
    "    print(f\"\\n{'EPISODE DURATION (STEPS)':^80}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Overall:\")\n",
    "    print(f\"    Mean:                  {np.mean(episode_steps):7.1f}\")\n",
    "    print(f\"    Median:                {np.median(episode_steps):7.1f}\")\n",
    "    print(f\"    Min:                   {min(episode_steps):7d}\")\n",
    "    print(f\"    Max:                   {max(episode_steps):7d}\")\n",
    "    \n",
    "    if success_steps_list:\n",
    "        print(f\"  Success Episodes:\")\n",
    "        print(f\"    Mean:                  {np.mean(success_steps_list):7.1f}\")\n",
    "        print(f\"    Median:                {np.median(success_steps_list):7.1f}\")\n",
    "        print(f\"    Min:                   {min(success_steps_list):7d}\")\n",
    "        print(f\"    Max:                   {max(success_steps_list):7d}\")\n",
    "    \n",
    "    if failure_steps_list:\n",
    "        print(f\"  Failure Episodes:\")\n",
    "        print(f\"    Mean:                  {np.mean(failure_steps_list):7.1f}\")\n",
    "        print(f\"    Median:                {np.median(failure_steps_list):7.1f}\")\n",
    "    \n",
    "    print(f\"\\n{'TIMING STATISTICS':^80}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Avg Episode Time:        {np.mean(episode_times):7.4f}s\")\n",
    "    print(f\"  Total Evaluation Time:   {total_time:7.2f}s\")\n",
    "    print(f\"  Episodes per Second:     {num_episodes/total_time:7.2f}\")\n",
    "    \n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DEMO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUICK EVALUATION (10 Episodes)\n",
      "================================================================================\n",
      "Checkpoint:    google_colab_checkpoints/sokoban-small-v0/ppo_sokoban_ep3000.pth\n",
      "Environment:   Sokoban-small-v0\n",
      "Episodes:      10\n",
      "Max Steps:     200\n",
      "================================================================================\n",
      "\n",
      "Episode    1/10 | Reward:  -19.00 | Steps: 200 | ✗ FAIL  | TIMEOUT\n",
      "Episode    2/10 | Reward:    2.80 | Steps:  92 | ✓ PASS  |        \n",
      "Episode    3/10 | Reward:  -20.00 | Steps: 200 | ✗ FAIL  | TIMEOUT\n",
      "Episode    4/10 | Reward:  -19.00 | Steps: 200 | ✗ FAIL  | TIMEOUT\n",
      "Episode    5/10 | Reward:  -20.00 | Steps: 200 | ✗ FAIL  | TIMEOUT\n",
      "Episode    6/10 | Reward:  -19.00 | Steps: 200 | ✗ FAIL  | TIMEOUT\n",
      "Episode    7/10 | Reward:  -20.00 | Steps: 200 | ✗ FAIL  | TIMEOUT\n",
      "Episode    8/10 | Reward:  -20.00 | Steps: 200 | ✗ FAIL  | TIMEOUT\n",
      "Episode    9/10 | Reward:  -20.00 | Steps: 200 | ✗ FAIL  | TIMEOUT\n",
      "Episode   10/10 | Reward:  -20.00 | Steps: 200 | ✗ FAIL  | TIMEOUT\n",
      "\n",
      "================================================================================\n",
      "FINAL EVALUATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "                              OVERALL PERFORMANCE                               \n",
      "--------------------------------------------------------------------------------\n",
      "  Total Episodes:          10\n",
      "  Successful Episodes:     1 (10.00%)\n",
      "  Failed Episodes:         9 (90.00%)\n",
      "  Timeout Episodes:        9 (90.00%)\n",
      "\n",
      "                               REWARD STATISTICS                                \n",
      "--------------------------------------------------------------------------------\n",
      "  Overall:\n",
      "    Mean:                   -17.42\n",
      "    Median:                 -20.00\n",
      "    Std Dev:                  6.75\n",
      "    Min:                    -20.00\n",
      "    Max:                      2.80\n",
      "  Success Episodes:\n",
      "    Mean:                     2.80\n",
      "    Median:                   2.80\n",
      "    Std Dev:                  0.00\n",
      "  Failure Episodes:\n",
      "    Mean:                   -19.67\n",
      "    Median:                 -20.00\n",
      "    Std Dev:                  0.47\n",
      "\n",
      "                            EPISODE DURATION (STEPS)                            \n",
      "--------------------------------------------------------------------------------\n",
      "  Overall:\n",
      "    Mean:                    189.2\n",
      "    Median:                  200.0\n",
      "    Min:                        92\n",
      "    Max:                       200\n",
      "  Success Episodes:\n",
      "    Mean:                     92.0\n",
      "    Median:                   92.0\n",
      "    Min:                        92\n",
      "    Max:                        92\n",
      "  Failure Episodes:\n",
      "    Mean:                    200.0\n",
      "    Median:                  200.0\n",
      "\n",
      "                               TIMING STATISTICS                                \n",
      "--------------------------------------------------------------------------------\n",
      "  Avg Episode Time:         2.9862s\n",
      "  Total Evaluation Time:     30.84s\n",
      "  Episodes per Second:        0.32\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run quick evaluation with 10 episodes\n",
    "quick_evaluation(\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    env_name='Sokoban-small-v0',\n",
    "    num_episodes=10,\n",
    "    max_steps=200,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_agent_pygame(checkpoint_path, env_name='Sokoban-small-v0', max_steps=200, fps=5, save_videos=True, output_dir='puzzle_videos', min_success_reward=10.0):\n",
    "    \"\"\"\n",
    "    Visualize trained agent solving Sokoban puzzles using Pygame.\n",
    "    Shows 2 episodes: 1 failure and 1 success (with reward > min_success_reward).\n",
    "    Optionally saves videos to files.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path: Path to trained agent checkpoint\n",
    "        env_name: Sokoban environment name\n",
    "        max_steps: Maximum steps per episode\n",
    "        fps: Frames per second for visualization and video\n",
    "        save_videos: If True, save episodes as video files (MP4)\n",
    "        output_dir: Directory to save videos\n",
    "        min_success_reward: Minimum reward required for success episode (default: 10.0)\n",
    "    \"\"\"\n",
    "    import pygame\n",
    "    import time\n",
    "    import imageio\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Create output directory if saving videos\n",
    "    if save_videos:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        print(f\"Videos will be saved to: {output_dir}/\")\n",
    "\n",
    "    # Create environment WITHOUT reward shaping for true performance\n",
    "    env = gym.make(env_name)\n",
    "    agent = PPOAgent(env)\n",
    "    agent.load(checkpoint_path)\n",
    "\n",
    "    # Initialize Pygame\n",
    "    pygame.init()\n",
    "\n",
    "    # Get initial render to determine window size\n",
    "    obs = env.reset()\n",
    "    img = env.render(mode='rgb_array')\n",
    "    scale = 3\n",
    "    screen = pygame.display.set_mode((img.shape[1] * scale, img.shape[0] * scale))\n",
    "    pygame.display.set_caption('Sokoban Agent Visualization - ep3000')\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    # Font for overlay text\n",
    "    font = pygame.font.Font(None, 32)\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"PYGAME VISUALIZATION STARTED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Looking for 1 failure and 1 success (reward > {min_success_reward}) to display...\")\n",
    "    print(\"Close window or press ESC to exit\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    episodes_to_show = []\n",
    "    target_outcomes = ['failure', 'success']\n",
    "    attempt_count = 0\n",
    "    max_attempts = 100  # Increased to 100 attempts to find good success episode\n",
    "\n",
    "    # Collect episodes with desired outcomes\n",
    "    while len(episodes_to_show) < 2 and attempt_count < max_attempts:\n",
    "        attempt_count += 1\n",
    "\n",
    "        state = env.reset()\n",
    "        if len(state.shape) == 3:\n",
    "            state = np.transpose(state, (2, 0, 1))\n",
    "\n",
    "        episode_data = {\n",
    "            'frames': [],\n",
    "            'rewards': [],\n",
    "            'steps': 0,\n",
    "            'outcome': None,\n",
    "            'total_reward': 0\n",
    "        }\n",
    "\n",
    "        done = False\n",
    "        steps = 0\n",
    "        episode_reward = 0\n",
    "\n",
    "        # Run episode and collect frames\n",
    "        while not done and steps < max_steps:\n",
    "            # Capture frame\n",
    "            frame = env.render(mode='rgb_array')\n",
    "            episode_data['frames'].append(frame)\n",
    "\n",
    "            # Agent takes action\n",
    "            action, _ = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            if len(next_state.shape) == 3:\n",
    "                next_state = np.transpose(next_state, (2, 0, 1))\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            episode_data['rewards'].append(episode_reward)\n",
    "\n",
    "        # Capture final frame\n",
    "        final_frame = env.render(mode='rgb_array')\n",
    "        episode_data['frames'].append(final_frame)\n",
    "        episode_data['rewards'].append(episode_reward)\n",
    "\n",
    "        # Determine outcome\n",
    "        success = done and info.get('all_boxes_on_target', False)\n",
    "        timeout = steps >= max_steps\n",
    "\n",
    "        episode_data['steps'] = steps\n",
    "        episode_data['total_reward'] = episode_reward\n",
    "        episode_data['outcome'] = 'success' if success else 'failure'\n",
    "\n",
    "        # Check if we need this outcome\n",
    "        needed_outcome = target_outcomes[len(episodes_to_show)]\n",
    "\n",
    "        # For success episodes, also check reward threshold\n",
    "        if needed_outcome == 'success':\n",
    "            if episode_data['outcome'] == 'success' and episode_reward > min_success_reward:\n",
    "                episodes_to_show.append(episode_data)\n",
    "                print(f\"Found {episode_data['outcome']} episode (attempt #{attempt_count}): \"\n",
    "                      f\"Reward={episode_reward:.2f}, Steps={steps} - ACCEPTED (reward > {min_success_reward})\")\n",
    "            elif episode_data['outcome'] == 'success':\n",
    "                print(f\"  Skipping success episode (attempt #{attempt_count}): \"\n",
    "                      f\"Reward={episode_reward:.2f} - TOO LOW (need > {min_success_reward})\")\n",
    "        else:\n",
    "            # For failure episodes, just match the outcome\n",
    "            if episode_data['outcome'] == needed_outcome:\n",
    "                episodes_to_show.append(episode_data)\n",
    "                print(f\"Found {episode_data['outcome']} episode (attempt #{attempt_count}): \"\n",
    "                      f\"Reward={episode_reward:.2f}, Steps={steps}\")\n",
    "\n",
    "    if len(episodes_to_show) < 2:\n",
    "        print(f\"\\nWarning: Could only find {len(episodes_to_show)} episodes after {max_attempts} attempts\")\n",
    "        print(\"Displaying what we found...\")\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STARTING VISUALIZATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Display the collected episodes\n",
    "    running = True\n",
    "    for ep_idx, episode_data in enumerate(episodes_to_show):\n",
    "        if not running:\n",
    "            break\n",
    "\n",
    "        episode_num = ep_idx + 1\n",
    "        outcome = episode_data['outcome']\n",
    "        total_steps = episode_data['steps']\n",
    "        total_reward = episode_data['total_reward']\n",
    "\n",
    "        print(f\"Episode {episode_num}/2: {outcome.upper()} \"\n",
    "              f\"(Reward: {total_reward:.2f}, Steps: {total_steps})\")\n",
    "\n",
    "        # Prepare video writer if saving\n",
    "        video_frames = []\n",
    "        if save_videos:\n",
    "            video_filename = f\"{output_dir}/episode_{episode_num}_{outcome}_{timestamp}.mp4\"\n",
    "\n",
    "        # Play through the episode frames\n",
    "        for step_idx, frame in enumerate(episode_data['frames']):\n",
    "            # Check for quit events\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    running = False\n",
    "                    break\n",
    "                elif event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_ESCAPE:\n",
    "                        running = False\n",
    "                        break\n",
    "\n",
    "            if not running:\n",
    "                break\n",
    "\n",
    "            # Render the game state\n",
    "            surf = pygame.surfarray.make_surface(np.transpose(frame, (1, 0, 2)))\n",
    "            surf = pygame.transform.scale(surf, (frame.shape[1] * scale, frame.shape[0] * scale))\n",
    "            screen.blit(surf, (0, 0))\n",
    "\n",
    "            # Draw semi-transparent overlay background for text\n",
    "            overlay = pygame.Surface((screen.get_width(), 100))\n",
    "            overlay.set_alpha(200)\n",
    "            overlay.fill((0, 0, 0))\n",
    "            screen.blit(overlay, (0, 0))\n",
    "\n",
    "            # Prepare overlay text\n",
    "            current_reward = episode_data['rewards'][step_idx] if step_idx < len(episode_data['rewards']) else total_reward\n",
    "            status = \"SUCCESS\" if outcome == 'success' and step_idx == len(episode_data['frames']) - 1 else                      \"FAILED\" if outcome == 'failure' and step_idx == len(episode_data['frames']) - 1 else                      \"IN PROGRESS\"\n",
    "\n",
    "            # Render text\n",
    "            text_line1 = font.render(f\"Episode: {episode_num}/2  |  Step: {step_idx}/{total_steps}\", True, (255, 255, 255))\n",
    "            text_line2 = font.render(f\"Reward: {current_reward:.2f}  |  Status: {status}\", True, (255, 255, 0))\n",
    "\n",
    "            # Blit text\n",
    "            screen.blit(text_line1, (10, 10))\n",
    "            screen.blit(text_line2, (10, 50))\n",
    "\n",
    "            pygame.display.flip()\n",
    "\n",
    "            # Capture frame for video\n",
    "            if save_videos:\n",
    "                # Get the rendered screen as RGB array\n",
    "                video_frame = pygame.surfarray.array3d(screen)\n",
    "                video_frame = np.transpose(video_frame, (1, 0, 2))  # Transpose to (height, width, channels)\n",
    "                video_frames.append(video_frame)\n",
    "\n",
    "            clock.tick(fps)\n",
    "\n",
    "        # Save video if enabled\n",
    "        if save_videos and video_frames:\n",
    "            print(f\"  Saving video: {video_filename}\")\n",
    "            imageio.mimsave(video_filename, video_frames, fps=fps, codec='libx264', quality=8)\n",
    "            print(f\"  Video saved successfully! ({len(video_frames)} frames)\")\n",
    "\n",
    "        # Pause between episodes (2 seconds)\n",
    "        if running and ep_idx < len(episodes_to_show) - 1:\n",
    "            print(\"  (Pausing 2 seconds before next episode...)\\n\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # Clean up\n",
    "    pygame.quit()\n",
    "    env.close()\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"VISUALIZATION COMPLETE\")\n",
    "    if save_videos:\n",
    "        print(f\"Videos saved to: {os.path.abspath(output_dir)}/\")\n",
    "    print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pygame Visualization\n",
    "\n",
    "Visual demonstration of agent solving puzzles.\n",
    "\n",
    "Features:\n",
    "- Shows 1 failure and 1 success episode visually\n",
    "- Saves videos to puzzle_videos/ folder\n",
    "- Requires pygame and imageio libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint and starting Pygame visualization...\n",
      "This will show 2 episodes: 1 failure and 1 success (reward > 10)\n",
      "Videos will be saved to puzzle_videos/ folder\n",
      "Close the Pygame window or press ESC to exit\n",
      "\n",
      "Videos will be saved to: puzzle_videos/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PYGAME VISUALIZATION STARTED\n",
      "================================================================================\n",
      "Looking for 1 failure and 1 success (reward > 9.0) to display...\n",
      "Close window or press ESC to exit\n",
      "================================================================================\n",
      "\n",
      "Found failure episode (attempt #1): Reward=-20.00, Steps=200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideos will be saved to puzzle_videos/ folder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClose the Pygame window or press ESC to exit\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mvisualize_agent_pygame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSokoban-small-v0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 5 frames per second for easy viewing\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_videos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to False to disable video saving\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpuzzle_videos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Folder where videos will be saved\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_success_reward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9.0\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Only show success episodes with reward > 10\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[65], line 87\u001b[0m, in \u001b[0;36mvisualize_agent_pygame\u001b[1;34m(checkpoint_path, env_name, max_steps, fps, save_videos, output_dir, min_success_reward)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Agent takes action\u001b[39;00m\n\u001b[0;32m     86\u001b[0m action, _ \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[1;32m---> 87\u001b[0m next_state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(next_state\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m     90\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(next_state, (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym_sokoban\\envs\\sokoban_env.py:80\u001b[0m, in \u001b[0;36mSokobanEnv.step\u001b[1;34m(self, action, observation_mode)\u001b[0m\n\u001b[0;32m     77\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_done()\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Convert the observation to RGB frame\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m info \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction.name\u001b[39m\u001b[38;5;124m\"\u001b[39m: ACTION_LOOKUP[action],\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction.moved_player\u001b[39m\u001b[38;5;124m\"\u001b[39m: moved_player,\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction.moved_box\u001b[39m\u001b[38;5;124m\"\u001b[39m: moved_box,\n\u001b[0;32m     86\u001b[0m }\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym_sokoban\\envs\\sokoban_env.py:225\u001b[0m, in \u001b[0;36mSokobanEnv.render\u001b[1;34m(self, mode, close, scale)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m, close\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m RENDERING_MODES\n\u001b[1;32m--> 225\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym_sokoban\\envs\\sokoban_env.py:245\u001b[0m, in \u001b[0;36mSokobanEnv.get_image\u001b[1;34m(self, mode, scale)\u001b[0m\n\u001b[0;32m    243\u001b[0m     img \u001b[38;5;241m=\u001b[39m room_to_tiny_world_rgb(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroom_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroom_fixed, scale\u001b[38;5;241m=\u001b[39mscale)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 245\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mroom_to_rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroom_fixed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\gym_sokoban\\envs\\render_utils.py:29\u001b[0m, in \u001b[0;36mroom_to_rgb\u001b[1;34m(room, room_structure)\u001b[0m\n\u001b[0;32m     26\u001b[0m box_on_target \u001b[38;5;241m=\u001b[39m imageio\u001b[38;5;241m.\u001b[39mimread(box_on_target_filename)\n\u001b[0;32m     28\u001b[0m box_target_filename \u001b[38;5;241m=\u001b[39m pkg_resources\u001b[38;5;241m.\u001b[39mresource_filename(resource_package, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurface\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbox_target.png\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m---> 29\u001b[0m box_target \u001b[38;5;241m=\u001b[39m \u001b[43mimageio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox_target_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m floor_filename \u001b[38;5;241m=\u001b[39m pkg_resources\u001b[38;5;241m.\u001b[39mresource_filename(resource_package, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurface\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloor.png\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m     32\u001b[0m floor \u001b[38;5;241m=\u001b[39m imageio\u001b[38;5;241m.\u001b[39mimread(floor_filename)\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\imageio\\__init__.py:100\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(uri, format, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"imread(uri, format=None, **kwargs)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mReads an image from the specified file. Returns a numpy array, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m    to see what arguments are available for a particular format.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting with ImageIO v3 the behavior of this function will switch to that of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m iio.v3.imread. To keep the current behavior (and make this warning disappear)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     98\u001b[0m )\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m imread_v2(uri, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\imageio\\v2.py:359\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(uri, format, **kwargs)\u001b[0m\n\u001b[0;32m    356\u001b[0m imopen_args \u001b[38;5;241m=\u001b[39m decypher_format_arg(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m    357\u001b[0m imopen_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mri\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimopen_args) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m    360\u001b[0m     result \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\imageio\\core\\imopen.py:196\u001b[0m, in \u001b[0;36mimopen\u001b[1;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m     plugin_instance \u001b[38;5;241m=\u001b[39m candidate_plugin(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InitializationError:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# file extension doesn't match file type\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\canid\\OneDrive\\Masaüstü\\sokoban_bs\\final_sub\\.venv\\lib\\site-packages\\imageio\\plugins\\pillow.py:95\u001b[0m, in \u001b[0;36mPillowPlugin.__init__\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Register AVIF opener for Pillow\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpillow_heif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_avif_opener\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1430\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1402\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1535\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run Pygame visualization with ep3000 checkpoint\n",
    "\n",
    "print(\"Loading checkpoint and starting Pygame visualization...\")\n",
    "print(\"This will show 2 episodes: 1 failure and 1 success (reward > 10)\")\n",
    "print(\"Videos will be saved to puzzle_videos/ folder\")\n",
    "print(\"Close the Pygame window or press ESC to exit\\n\")\n",
    "\n",
    "visualize_agent_pygame(\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    env_name='Sokoban-small-v0',\n",
    "    max_steps=200,\n",
    "    fps=5,  # 5 frames per second for easy viewing\n",
    "    save_videos=True,  # Set to False to disable video saving\n",
    "    output_dir='puzzle_videos',  # Folder where videos will be saved\n",
    "    min_success_reward=9.0  # Only show success episodes with reward > 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
