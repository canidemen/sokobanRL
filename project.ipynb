{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sokoban RL with PPO\n",
        "\n",
        "**CS 175 Final Project** - Training a PPO agent for Sokoban-small-v0\n",
        "\n",
        "## Quick Start\n",
        "\n",
        "1. **Run cells 1-9** (setup and class definitions)\n",
        "2. **Run Cell 10 (DEMO)** - Quick demonstration with pre-trained agent\n",
        "3. Optionally view Cell 11 for comprehensive evaluation\n",
        "\n",
        "**Note:** Training from scratch (Cell 12) takes several hours.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\canid\\OneDrive\\Masaüstü\\Python\\CS_175\\sokobanRL\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\canid\\OneDrive\\Masaüstü\\Python\\CS_175\\sokobanRL\\.venv\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "!pip install -q -r requirements.txt\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymimport gym_sokobanimport pygameimport numpy as npimport torchimport torch.nn as nnimport torch.optim as optimfrom collections import dequeimport matplotlib.pyplot as pltimport os# NumPy 2.x compatibility patchif not hasattr(np, 'bool8'):    np.bool8 = np.bool_# Check if GPU is availabledevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")print(f\"Device: {device}\")# Reward Shaping Wrapper - BALANCED for stable learningclass SokobanRewardShaper(gym.Wrapper):    \"\"\"Reward shaping wrapper for Sokoban-small-v0.\"\"\"    def __init__(self, env):        super().__init__(env)        self.previous_min_distance = None        self.boxes_on_target = 0        self.max_steps = 150  # Should match max_timesteps in training    def reset(self, **kwargs):        obs = self.env.reset(**kwargs)        self.previous_min_distance = self._compute_min_box_target_distance()        self.boxes_on_target = self._count_boxes_on_target()        return obs    def _compute_min_box_target_distance(self):        \"\"\"Compute sum of minimum distances from each box to nearest target\"\"\"        room = self.env.unwrapped.room_state        # Find box positions (tile value 3 = box on target, 4 = box off target)        boxes = np.argwhere((room == 3) | (room == 4))        # Find target positions (tile value 2 = empty target, 3 = box on target)        targets = np.argwhere((room == 2) | (room == 3))        if len(boxes) == 0 or len(targets) == 0:            return 0        # Calculate sum of minimum Manhattan distances        total_min_dist = 0        for box in boxes:            # Manhattan distance to each target            distances = np.abs(targets - box).sum(axis=1)            min_dist = distances.min()            total_min_dist += min_dist        return total_min_dist    def _count_boxes_on_target(self):        \"\"\"Count boxes on target positions (tile value 3)\"\"\"        room = self.env.unwrapped.room_state        return np.sum(room == 3)    def step(self, action):        obs, reward, done, info = self.env.step(action)        # Start with base reward        shaped_reward = reward        # REDUCED reward for moving boxes closer to targets        current_min_distance = self._compute_min_box_target_distance()        if self.previous_min_distance is not None:            distance_change = self.previous_min_distance - current_min_distance            if distance_change > 0:  # Moved closer                shaped_reward += 0.1 * distance_change  # REDUCED from 0.5            elif distance_change < 0:  # Moved away                shaped_reward += 0.1 * distance_change  # Small penalty        self.previous_min_distance = current_min_distance        # REDUCED reward for placing box on target        current_boxes_on_target = self._count_boxes_on_target()        if current_boxes_on_target > self.boxes_on_target:            shaped_reward += 0.5 * (current_boxes_on_target - self.boxes_on_target)  # REDUCED from 2.0        self.boxes_on_target = current_boxes_on_target                # NEW: Penalty for timeout without completion (prevents reward farming)        if done and not info.get('all_boxes_on_target', False):            # Check if we hit max steps            if hasattr(self.env.unwrapped, 'num_env_steps'):                if self.env.unwrapped.num_env_steps >= self.max_steps - 1:                    shaped_reward -= 3.0  # Timeout penalty        return obs, shaped_reward, done, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Actor-Critic Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        \n",
        "        # Convolutional layers with Layer Normalization for stability\n",
        "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=3, stride=2, padding=1)\n",
        "        self.ln1 = nn.LayerNorm([32, input_shape[1]//2, input_shape[2]//2])\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.ln2 = nn.LayerNorm([64, input_shape[1]//4, input_shape[2]//4])\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.ln3 = nn.LayerNorm([64, input_shape[1]//8, input_shape[2]//8])\n",
        "        \n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        \n",
        "        # Actor head (policy) with smaller hidden layer\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 128),  # Reduced from 256\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, n_actions)\n",
        "        )\n",
        "        \n",
        "        # Critic head (value function) with smaller hidden layer\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 128),  # Reduced from 256\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        \n",
        "        # Orthogonal initialization for better gradient flow\n",
        "        self._initialize_weights()\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize weights using orthogonal initialization\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "    \n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self._forward_conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "    \n",
        "    def _forward_conv(self, x):\n",
        "        \"\"\"Forward pass through convolutional layers\"\"\"\n",
        "        x = torch.relu(self.ln1(self.conv1(x)))\n",
        "        x = torch.relu(self.ln2(self.conv2(x)))\n",
        "        x = torch.relu(self.ln3(self.conv3(x)))\n",
        "        return x\n",
        "    \n",
        "    def forward(self, x):\n",
        "        conv_out = self._forward_conv(x).view(x.size()[0], -1)\n",
        "        return self.actor(conv_out), self.critic(conv_out)\n",
        "    \n",
        "    def get_action_probs(self, x):\n",
        "        logits, _ = self.forward(x)\n",
        "        return torch.softmax(logits, dim=-1)\n",
        "    \n",
        "    def get_value(self, x):\n",
        "        _, value = self.forward(x)\n",
        "        return value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PPO Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PPOAgent:\n",
        "    def __init__(self, env, lr=3e-4, gamma=0.99, eps_clip=0.2, K_epochs=4, gae_lambda=0.95, \n",
        "                 entropy_coef=0.05, value_clip=0.2, warmup_steps=10, advantage_clip=10.0):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.value_clip = value_clip\n",
        "        self.advantage_clip = advantage_clip\n",
        "        \n",
        "        # Learning rate warmup\n",
        "        self.base_lr = lr\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.current_update = 0\n",
        "        \n",
        "        # Get observation shape\n",
        "        obs = env.reset()\n",
        "        if len(obs.shape) == 3:\n",
        "            obs = np.transpose(obs, (2, 0, 1))\n",
        "        \n",
        "        self.input_shape = obs.shape\n",
        "        self.n_actions = env.action_space.n\n",
        "        \n",
        "        self.device = torch.device(\"cpu\")\n",
        "        self.policy = ActorCritic(self.input_shape, self.n_actions).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        \n",
        "        self.policy_old = ActorCritic(self.input_shape, self.n_actions).to(self.device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        \n",
        "        self.MseLoss = nn.MSELoss()\n",
        "    \n",
        "    def _get_current_lr(self):\n",
        "        \"\"\"Get learning rate with warmup schedule\"\"\"\n",
        "        if self.current_update < self.warmup_steps:\n",
        "            # Linear warmup from 0 to base_lr\n",
        "            return self.base_lr * (self.current_update + 1) / self.warmup_steps\n",
        "        else:\n",
        "            return self.base_lr\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            action_probs = self.policy_old.get_action_probs(state)\n",
        "        \n",
        "        dist = torch.distributions.Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        \n",
        "        return action.item(), action_logprob.item()\n",
        "    \n",
        "    def compute_gae(self, rewards, values, dones):\n",
        "        advantages = []\n",
        "        gae = 0\n",
        "        \n",
        "        for t in reversed(range(len(rewards))):\n",
        "            if t == len(rewards) - 1:\n",
        "                next_value = 0\n",
        "            else:\n",
        "                next_value = values[t + 1]\n",
        "            \n",
        "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
        "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
        "            advantages.insert(0, gae)\n",
        "        \n",
        "        return advantages\n",
        "    \n",
        "    def update(self, memory):\n",
        "        self.current_update += 1\n",
        "        current_lr = self._get_current_lr()\n",
        "        \n",
        "        # Update learning rate\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = current_lr\n",
        "        \n",
        "        states = torch.FloatTensor(np.array(memory['states'])).to(self.device)\n",
        "        actions = torch.LongTensor(memory['actions']).to(self.device)\n",
        "        old_logprobs = torch.FloatTensor(memory['logprobs']).to(self.device)\n",
        "        \n",
        "        rewards = memory['rewards']\n",
        "        dones = memory['dones']\n",
        "        \n",
        "        # Compute values and advantages\n",
        "        with torch.no_grad():\n",
        "            old_values = self.policy_old.get_value(states).squeeze().cpu().numpy()\n",
        "        \n",
        "        advantages = self.compute_gae(rewards, old_values, dones)\n",
        "        advantages_tensor = torch.FloatTensor(advantages).to(self.device)\n",
        "        \n",
        "        # Store raw advantage statistics before clipping/normalization\n",
        "        raw_adv_mean = advantages_tensor.mean().item()\n",
        "        raw_adv_std = advantages_tensor.std().item()\n",
        "        raw_adv_max = advantages_tensor.max().item()\n",
        "        raw_adv_min = advantages_tensor.min().item()\n",
        "        \n",
        "        # CLIP advantages to prevent extreme values\n",
        "        advantages_tensor = torch.clamp(advantages_tensor, -self.advantage_clip, self.advantage_clip)\n",
        "        \n",
        "        # Normalize advantages\n",
        "        advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)\n",
        "        \n",
        "        returns = advantages_tensor + torch.FloatTensor(old_values).to(self.device)\n",
        "        old_values_tensor = torch.FloatTensor(old_values).to(self.device)\n",
        "        \n",
        "        # Optimize policy for K epochs\n",
        "        total_grad_norm = 0.0\n",
        "        for _ in range(self.K_epochs):\n",
        "            logits, state_values = self.policy(states)\n",
        "            dist = torch.distributions.Categorical(logits=logits)\n",
        "            action_logprobs = dist.log_prob(actions)\n",
        "            dist_entropy = dist.entropy()\n",
        "            \n",
        "            ratios = torch.exp(action_logprobs - old_logprobs)\n",
        "            \n",
        "            surr1 = ratios * advantages_tensor\n",
        "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages_tensor\n",
        "            \n",
        "            actor_loss = -torch.min(surr1, surr2).mean()\n",
        "            \n",
        "            # Value function loss with clipping\n",
        "            state_values_squeeze = state_values.squeeze()\n",
        "            value_pred_clipped = old_values_tensor + torch.clamp(\n",
        "                state_values_squeeze - old_values_tensor,\n",
        "                -self.value_clip,\n",
        "                self.value_clip\n",
        "            )\n",
        "            value_loss1 = self.MseLoss(state_values_squeeze, returns)\n",
        "            value_loss2 = self.MseLoss(value_pred_clipped, returns)\n",
        "            critic_loss = torch.max(value_loss1, value_loss2)\n",
        "            \n",
        "            entropy_loss = -self.entropy_coef * dist_entropy.mean()\n",
        "            \n",
        "            loss = actor_loss + 0.5 * critic_loss + entropy_loss\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
        "            total_grad_norm += grad_norm.item()\n",
        "            self.optimizer.step()\n",
        "        \n",
        "        avg_grad_norm = total_grad_norm / self.K_epochs\n",
        "        \n",
        "        # Calculate policy ratio statistics\n",
        "        with torch.no_grad():\n",
        "            final_logits, _ = self.policy(states)\n",
        "            final_dist = torch.distributions.Categorical(logits=final_logits)\n",
        "            final_logprobs = final_dist.log_prob(actions)\n",
        "            final_ratios = torch.exp(final_logprobs - old_logprobs)\n",
        "            \n",
        "            ratio_mean = final_ratios.mean().item()\n",
        "            ratio_std = final_ratios.std().item()\n",
        "            ratio_max = final_ratios.max().item()\n",
        "            ratio_min = final_ratios.min().item()\n",
        "        \n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        \n",
        "        # Return comprehensive metrics\n",
        "        metrics = {\n",
        "            'actor_loss': actor_loss.item(),\n",
        "            'critic_loss': critic_loss.item(),\n",
        "            'entropy': -entropy_loss.item() / self.entropy_coef,\n",
        "            'grad_norm': avg_grad_norm,\n",
        "            'advantage_mean': raw_adv_mean,\n",
        "            'advantage_std': raw_adv_std,\n",
        "            'advantage_max': raw_adv_max,\n",
        "            'advantage_min': raw_adv_min,\n",
        "            'ratio_mean': ratio_mean,\n",
        "            'ratio_std': ratio_std,\n",
        "            'ratio_max': ratio_max,\n",
        "            'ratio_min': ratio_min,\n",
        "            'value_mean': np.mean(old_values),\n",
        "            'value_std': np.std(old_values),\n",
        "            'learning_rate': current_lr,\n",
        "        }\n",
        "        \n",
        "        return metrics\n",
        "    \n",
        "    def save(self, path):\n",
        "        torch.save(self.policy.state_dict(), path)\n",
        "    \n",
        "    def load(self, path):\n",
        "        self.policy.load_state_dict(torch.load(path))\n",
        "        self.policy_old.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(env_name='Sokoban-v0', max_episodes=10000, max_timesteps=300, update_timestep=2048, save_freq=100, \n",
        "          resume_from_checkpoint=None, start_episode=1):\n",
        "    import datetime\n",
        "    \n",
        "    # Enable reward shaping for sparse reward exploration\n",
        "    env = gym.make(env_name)\n",
        "    env = SokobanRewardShaper(env)  # Enable reward shaping\n",
        "    \n",
        "    agent = PPOAgent(env)\n",
        "    \n",
        "    # Load checkpoint if provided\n",
        "    if resume_from_checkpoint is not None:\n",
        "        print(f\"\\n{'='*100}\")\n",
        "        print(f\"RESUMING FROM CHECKPOINT: {resume_from_checkpoint}\")\n",
        "        print(f\"Starting from episode: {start_episode}\")\n",
        "        print(f\"{'='*100}\\n\")\n",
        "        agent.load(resume_from_checkpoint)\n",
        "    \n",
        "    os.makedirs('checkpoints', exist_ok=True)\n",
        "    os.makedirs('logs', exist_ok=True)\n",
        "    \n",
        "    # Create log file with timestamp\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    if resume_from_checkpoint:\n",
        "        log_file = f'logs/training_log_resumed_{timestamp}.txt'\n",
        "    else:\n",
        "        log_file = f'logs/training_log_{timestamp}.txt'\n",
        "    \n",
        "    # Write header to log file\n",
        "    with open(log_file, 'w') as f:\n",
        "        f.write(\"=\" * 100 + \"\\n\")\n",
        "        if resume_from_checkpoint:\n",
        "            f.write(f\"SOKOBAN PPO TRAINING LOG (RESUMED) - Started at {datetime.datetime.now()}\\n\")\n",
        "            f.write(f\"Resumed from checkpoint: {resume_from_checkpoint}\\n\")\n",
        "            f.write(f\"Starting episode: {start_episode}\\n\")\n",
        "        else:\n",
        "            f.write(f\"SOKOBAN PPO TRAINING LOG (WITH REWARD SHAPING) - Started at {datetime.datetime.now()}\\n\")\n",
        "        f.write(\"=\" * 100 + \"\\n\")\n",
        "        f.write(f\"Environment: {env_name} (WITH REWARD SHAPING)\\n\")\n",
        "        f.write(f\"Max Episodes: {max_episodes}\\n\")\n",
        "        f.write(f\"Max Timesteps per Episode: {max_timesteps}\\n\")\n",
        "        f.write(f\"Update Timestep: {update_timestep}\\n\")\n",
        "        f.write(f\"Save Frequency: {save_freq}\\n\")\n",
        "        f.write(\"=\" * 100 + \"\\n\\n\")\n",
        "        f.write(\"HYPERPARAMETER IMPROVEMENTS + REWARD SHAPING:\\n\")\n",
        "        f.write(\"1. REWARD SHAPING ENABLED: Intermediate rewards for box movements\\n\")\n",
        "        f.write(\"   - +0.5 for each unit a box moves closer to targets\\n\")\n",
        "        f.write(\"   - +2.0 for placing a box on a target\\n\")\n",
        "        f.write(\"   - -0.2 for moving a box away from targets\\n\")\n",
        "        f.write(\"2. Layer Normalization: Stabilizes network activations\\n\")\n",
        "        f.write(\"3. Orthogonal Initialization: Better gradient flow\\n\")\n",
        "        f.write(\"4. Value Function Clipping: Prevents critic divergence\\n\")\n",
        "        f.write(\"5. Learning Rate Warmup: Gradual increase (10 updates)\\n\")\n",
        "        f.write(\"6. Advantage Clipping: Prevents extreme advantage values\\n\")\n",
        "        f.write(\"7. IMPROVED Hyperparameters:\\n\")\n",
        "        f.write(\"   - lr=3e-4 (standard PPO learning rate)\\n\")\n",
        "        f.write(\"   - entropy_coef=0.05 (exploration)\\n\")\n",
        "        f.write(\"   - grad_clip=0.5 (stable gradients)\\n\")\n",
        "        f.write(\"   - K_epochs=4\\n\")\n",
        "        f.write(\"   - warmup_steps=10\\n\")\n",
        "        f.write(\"=\" * 100 + \"\\n\\n\")\n",
        "        f.write(\"METRICS EXPLANATION:\\n\")\n",
        "        f.write(\"- Episode: Episode number\\n\")\n",
        "        f.write(\"- Reward: Total reward for this episode (WITH SHAPING BONUSES)\\n\")\n",
        "        f.write(\"- Running Reward: Exponential moving average of rewards\\n\")\n",
        "        f.write(\"- Steps: Number of steps taken in this episode\\n\")\n",
        "        f.write(\"- Timestep: Total timesteps so far\\n\")\n",
        "        f.write(\"- Actor Loss: Policy improvement metric\\n\")\n",
        "        f.write(\"- Critic Loss: Value estimation error\\n\")\n",
        "        f.write(\"- Entropy: Action randomness (target: 0.5-2.0)\\n\")\n",
        "        f.write(\"- Grad Norm: Gradient magnitude\\n\")\n",
        "        f.write(\"- Learning Rate: Current LR with warmup\\n\")\n",
        "        f.write(\"- Ratio Mean: Policy change (should stay near 1.0)\\n\")\n",
        "        f.write(\"=\" * 100 + \"\\n\\n\")\n",
        "    \n",
        "    print(f\"Logging to: {log_file}\\n\")\n",
        "    print(\"REWARD SHAPING ENABLED:\")\n",
        "    print(\"  - +0.5 per unit boxes move closer to targets\")\n",
        "    print(\"  - +2.0 for placing box on target\")\n",
        "    print(\"  - -0.2 per unit boxes move away from targets\")\n",
        "    print(\"\\nIMPROVED HYPERPARAMETERS ACTIVE:\")\n",
        "    print(\"  - Layer normalization + Orthogonal init\")\n",
        "    print(\"  - Value function clipping + Advantage clipping (±10)\")\n",
        "    print(\"  - Learning rate warmup (10 updates)\")\n",
        "    print(\"  - lr=3e-4, entropy=0.05, grad_clip=0.5, K_epochs=4\\n\")\n",
        "    \n",
        "    episode_rewards = []\n",
        "    episode_steps = []\n",
        "    running_reward = 0\n",
        "    timestep = 0\n",
        "    \n",
        "    # Track latest update metrics\n",
        "    latest_metrics = None\n",
        "    \n",
        "    memory = {\n",
        "        'states': [],\n",
        "        'actions': [],\n",
        "        'logprobs': [],\n",
        "        'rewards': [],\n",
        "        'dones': []\n",
        "    }\n",
        "    \n",
        "    for episode in range(start_episode, max_episodes + 1):\n",
        "        state = env.reset()\n",
        "        if len(state.shape) == 3:\n",
        "            state = np.transpose(state, (2, 0, 1))\n",
        "        \n",
        "        episode_reward = 0\n",
        "        \n",
        "        for t in range(max_timesteps):\n",
        "            timestep += 1\n",
        "            \n",
        "            action, action_logprob = agent.select_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            \n",
        "            if len(next_state.shape) == 3:\n",
        "                next_state = np.transpose(next_state, (2, 0, 1))\n",
        "            \n",
        "            memory['states'].append(state)\n",
        "            memory['actions'].append(action)\n",
        "            memory['logprobs'].append(action_logprob)\n",
        "            memory['rewards'].append(reward)\n",
        "            memory['dones'].append(done)\n",
        "            \n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            \n",
        "            if timestep % update_timestep == 0:\n",
        "                latest_metrics = agent.update(memory)\n",
        "                memory = {\n",
        "                    'states': [],\n",
        "                    'actions': [],\n",
        "                    'logprobs': [],\n",
        "                    'rewards': [],\n",
        "                    'dones': []\n",
        "                }\n",
        "                print(f\"[UPDATE {agent.current_update}] Timestep {timestep} - \"\n",
        "                      f\"LR: {latest_metrics['learning_rate']:.2e}, \"\n",
        "                      f\"Actor: {latest_metrics['actor_loss']:.4f}, \"\n",
        "                      f\"Critic: {latest_metrics['critic_loss']:.4f}, \"\n",
        "                      f\"Entropy: {latest_metrics['entropy']:.4f}, \"\n",
        "                      f\"GradNorm: {latest_metrics['grad_norm']:.4f}\")\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_steps.append(t + 1)\n",
        "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "        \n",
        "        # Console output\n",
        "        print(f\"Episode {episode:5d} | Reward: {episode_reward:7.2f} | \"\n",
        "              f\"Running: {running_reward:7.2f} | Steps: {t+1:3d}\")\n",
        "        \n",
        "        # Write to log file after EVERY episode\n",
        "        with open(log_file, 'a') as f:\n",
        "            f.write(f\"\\n{'='*100}\\n\")\n",
        "            f.write(f\"EPISODE {episode} (Timestep: {timestep})\\n\")\n",
        "            f.write(f\"{'='*100}\\n\")\n",
        "            f.write(f\"  Reward:          {episode_reward:10.4f}\\n\")\n",
        "            f.write(f\"  Running Reward:  {running_reward:10.4f}\\n\")\n",
        "            f.write(f\"  Steps:           {t+1:10d}\\n\")\n",
        "            f.write(f\"  Total Timestep:  {timestep:10d}\\n\")\n",
        "            \n",
        "            # Add update metrics if available\n",
        "            if latest_metrics is not None:\n",
        "                f.write(f\"\\n  --- Latest Update Metrics (Update #{agent.current_update}) ---\\n\")\n",
        "                f.write(f\"  Learning Rate:   {latest_metrics['learning_rate']:10.8f}  (with warmup)\\n\")\n",
        "                f.write(f\"  Actor Loss:      {latest_metrics['actor_loss']:10.6f}\\n\")\n",
        "                f.write(f\"  Critic Loss:     {latest_metrics['critic_loss']:10.6f}\\n\")\n",
        "                f.write(f\"  Entropy:         {latest_metrics['entropy']:10.6f}\\n\")\n",
        "                f.write(f\"  Grad Norm:       {latest_metrics['grad_norm']:10.6f}\\n\")\n",
        "                f.write(f\"  \\n\")\n",
        "                f.write(f\"  Advantage Mean:  {latest_metrics['advantage_mean']:10.6f}\\n\")\n",
        "                f.write(f\"  Advantage Std:   {latest_metrics['advantage_std']:10.6f}\\n\")\n",
        "                f.write(f\"  Advantage Max:   {latest_metrics['advantage_max']:10.6f}\\n\")\n",
        "                f.write(f\"  Advantage Min:   {latest_metrics['advantage_min']:10.6f}\\n\")\n",
        "                f.write(f\"  \\n\")\n",
        "                f.write(f\"  Ratio Mean:      {latest_metrics['ratio_mean']:10.6f}\\n\")\n",
        "                f.write(f\"  Ratio Std:       {latest_metrics['ratio_std']:10.6f}\\n\")\n",
        "                f.write(f\"  Ratio Max:       {latest_metrics['ratio_max']:10.6f}\\n\")\n",
        "                f.write(f\"  Ratio Min:       {latest_metrics['ratio_min']:10.6f}\\n\")\n",
        "                f.write(f\"  \\n\")\n",
        "                f.write(f\"  Value Mean:      {latest_metrics['value_mean']:10.6f}\\n\")\n",
        "                f.write(f\"  Value Std:       {latest_metrics['value_std']:10.6f}\\n\")\n",
        "        \n",
        "        # Save checkpoints\n",
        "        if episode % save_freq == 0:\n",
        "            agent.save(f'checkpoints/ppo_sokoban_ep{episode}.pth')\n",
        "            print(f\"[CHECKPOINT] Model saved at episode {episode}\")\n",
        "            \n",
        "            with open(log_file, 'a') as f:\n",
        "                f.write(f\"\\n  >>> CHECKPOINT SAVED: checkpoints/ppo_sokoban_ep{episode}.pth\\n\")\n",
        "    \n",
        "    env.close()\n",
        "    \n",
        "    # Final summary\n",
        "    with open(log_file, 'a') as f:\n",
        "        f.write(f\"\\n\\n{'='*100}\\n\")\n",
        "        f.write(f\"TRAINING COMPLETED - {datetime.datetime.now()}\\n\")\n",
        "        f.write(f\"{'='*100}\\n\")\n",
        "        f.write(f\"Total Episodes:       {max_episodes - start_episode + 1}\\n\")\n",
        "        f.write(f\"Total Timesteps:      {timestep}\\n\")\n",
        "        f.write(f\"Final Running Reward: {running_reward:.4f}\\n\")\n",
        "        if episode_rewards:\n",
        "            f.write(f\"Best Episode Reward:  {max(episode_rewards):.4f} (Episode {episode_rewards.index(max(episode_rewards)) + start_episode})\\n\")\n",
        "            f.write(f\"Worst Episode Reward: {min(episode_rewards):.4f} (Episode {episode_rewards.index(min(episode_rewards)) + start_episode})\\n\")\n",
        "            f.write(f\"Average Reward:       {np.mean(episode_rewards):.4f}\\n\")\n",
        "            f.write(f\"Average Steps:        {np.mean(episode_steps):.2f}\\n\")\n",
        "        f.write(f\"{'='*100}\\n\")\n",
        "    \n",
        "    print(f\"\\nTraining complete! Log saved to: {log_file}\")\n",
        "    \n",
        "    return episode_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resume Training from Checkpoint\n",
        "\n",
        "**Instructions for Google Colab:**\n",
        "\n",
        "1. Upload your checkpoint file `ppo_sokoban_ep3000.pth` to `/content/checkpoints/` directory\n",
        "2. The training will resume from episode 3001\n",
        "3. New checkpoints will be saved every 100 episodes (ep3100, ep3200, etc.)\n",
        "4. Training will continue until episode 10000\n",
        "\n",
        "**Note:** Make sure you've uploaded the checkpoint file before running the cell below!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DEMO - Quick Evaluation (For Graders)\n",
        "\n",
        "This cell provides a quick demonstration of the pre-trained agent.\n",
        "Expected runtime: ~30 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEMO: Quick evaluation of pre-trained agent\n",
        "checkpoint_path = r'c:\\Users\\canid\\OneDrive\\Masaüstü\\Python\\CS_175\\sokobanRL\\google_colab_checkpoints\\sokoban-small-v0\\ppo_sokoban_ep3000.pth'\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DEMO: Testing Pre-Trained Agent (10 episodes)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "env = gym.make('Sokoban-small-v0')\n",
        "agent = PPOAgent(env)\n",
        "agent.load(checkpoint_path)\n",
        "\n",
        "successes = 0\n",
        "total_reward = 0\n",
        "\n",
        "for ep in range(10):\n",
        "    state = env.reset()\n",
        "    if len(state.shape) == 3:\n",
        "        state = np.transpose(state, (2, 0, 1))\n",
        "    \n",
        "    done = False\n",
        "    steps = 0\n",
        "    ep_reward = 0\n",
        "    \n",
        "    while not done and steps < 200:\n",
        "        action, _ = agent.select_action(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        if len(next_state.shape) == 3:\n",
        "            next_state = np.transpose(next_state, (2, 0, 1))\n",
        "        state = next_state\n",
        "        ep_reward += reward\n",
        "        steps += 1\n",
        "    \n",
        "    success = done and info.get('all_boxes_on_target', False)\n",
        "    if success:\n",
        "        successes += 1\n",
        "    total_reward += ep_reward\n",
        "    \n",
        "    status = \"✓ SUCCESS\" if success else \"✗ FAILED\"\n",
        "    print(f\"Episode {ep+1}/10: {status} | Reward: {ep_reward:6.2f} | Steps: {steps}\")\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DEMO RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Success Rate:    {successes}/10 ({successes*10}%)\")\n",
        "print(f\"Average Reward:  {total_reward/10:.2f}\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nFor full evaluation, run Cell 11 below.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resume training from ep3000 checkpoint on Sokoban-small-v0\n",
        "# Make sure the checkpoint file ppo_sokoban_ep3000.pth is uploaded to /content/checkpoints/ in Google Colab\n",
        "\n",
        "episode_rewards = train(\n",
        "    env_name='Sokoban-small-v0',\n",
        "    max_episodes=10000,              # Continue training until episode 10000\n",
        "    max_timesteps=150,               # Shorter episodes for smaller puzzles\n",
        "    update_timestep=2048,\n",
        "    save_freq=100,                   # Save checkpoint every 100 episodes\n",
        "    resume_from_checkpoint='/content/checkpoints/ppo_sokoban_ep3000.pth',  # Path to uploaded checkpoint\n",
        "    start_episode=3001               # Continue from episode 3001\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training rewards\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(episode_rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('Episode Rewards')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# Plot moving average\n",
        "window_size = 100\n",
        "if len(episode_rewards) >= window_size:\n",
        "    moving_avg = np.convolve(episode_rewards, np.ones(window_size)/window_size, mode='valid')\n",
        "    plt.plot(moving_avg)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Average Reward')\n",
        "    plt.title(f'Moving Average (window={window_size})')\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Trained Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_agent(checkpoint_path, env_name='Sokoban-small-v0', num_episodes=100, max_steps=200, render=False):\n",
        "    \"\"\"\n",
        "    Test a trained agent without reward shaping.\n",
        "    \n",
        "    Args:\n",
        "        checkpoint_path: Path to the checkpoint file\n",
        "        env_name: Environment to test on\n",
        "        num_episodes: Number of episodes to test\n",
        "        max_steps: Maximum steps per episode (default: 200)\n",
        "        render: Whether to render the environment\n",
        "    \n",
        "    Returns:\n",
        "        List of episode rewards (base Sokoban rewards without shaping)\n",
        "    \"\"\"\n",
        "    # Create environment WITHOUT reward shaping for true performance\n",
        "    env = gym.make(env_name)\n",
        "    agent = PPOAgent(env)\n",
        "    agent.load(checkpoint_path)\n",
        "    \n",
        "    total_rewards = []\n",
        "    success_count = 0\n",
        "    timeout_count = 0\n",
        "    \n",
        "    print(f\"Testing agent for {num_episodes} episodes on {env_name} (WITHOUT reward shaping)...\\n\")\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        if len(state.shape) == 3:\n",
        "            state = np.transpose(state, (2, 0, 1))\n",
        "        \n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            if render:\n",
        "                env.render()\n",
        "            \n",
        "            action, _ = agent.select_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            \n",
        "            if len(next_state.shape) == 3:\n",
        "                next_state = np.transpose(next_state, (2, 0, 1))\n",
        "            \n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "        \n",
        "        # Check success/failure status\n",
        "        success = done and info.get('all_boxes_on_target', False)\n",
        "        timeout = steps >= max_steps\n",
        "        \n",
        "        if success:\n",
        "            success_count += 1\n",
        "            status = \"✓ PASS\"\n",
        "        else:\n",
        "            status = \"✗ FAIL\"\n",
        "        \n",
        "        if timeout:\n",
        "            timeout_count += 1\n",
        "        \n",
        "        total_rewards.append(episode_reward)\n",
        "        \n",
        "        # Print every episode with pass/fail status\n",
        "        print(f\"Episode {episode + 1:3d}/{num_episodes} | Reward: {episode_reward:7.2f} | Steps: {steps:3d} | {status} | {'TIMEOUT' if timeout else ''}\")\n",
        "    \n",
        "    env.close()\n",
        "    \n",
        "    success_rate = (success_count / num_episodes) * 100\n",
        "    timeout_rate = (timeout_count / num_episodes) * 100\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"FINAL RESULTS:\")\n",
        "    print(f\"  Total Episodes:  {num_episodes}\")\n",
        "    print(f\"  Passed:          {success_count} ({success_rate:.1f}%)\")\n",
        "    print(f\"  Failed:          {num_episodes - success_count} ({100 - success_rate:.1f}%)\")\n",
        "    print(f\"  Timeouts:        {timeout_count} ({timeout_rate:.1f}%)\")\n",
        "    print(f\"  Average Reward:  {np.mean(total_rewards):.2f}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    return total_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the ep3000 checkpoint from Google Colab on Sokoban-small-v0\n",
        "checkpoint_path = r'c:\\Users\\canid\\OneDrive\\Masaüstü\\Python\\CS_175\\sokobanRL\\google_colab_checkpoints\\sokoban-small-v0\\ppo_sokoban_ep3000.pth'\n",
        "\n",
        "print(f\"Loading checkpoint: {checkpoint_path}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test on Sokoban-small-v0 (100 episodes for comprehensive evaluation)\n",
        "test_rewards = test_agent(checkpoint_path, env_name='Sokoban-small-v0', num_episodes=100, render=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PERFORMANCE SUMMARY (Sokoban-small-v0 - ep3000)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Episodes tested: 100\")\n",
        "print(f\"Average reward: {np.mean(test_rewards):.2f}\")\n",
        "print(f\"Best reward: {max(test_rewards):.2f}\")\n",
        "print(f\"Worst reward: {min(test_rewards):.2f}\")\n",
        "print(f\"Std deviation: {np.std(test_rewards):.2f}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Show distribution\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(test_rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('Test Episode Rewards on Sokoban-small-v0 (ep3000)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(test_rewards, bins=20, edgecolor='black')\n",
        "plt.xlabel('Reward')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Reward Distribution')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def comprehensive_evaluation(checkpoint_path, env_name='Sokoban-small-v0', num_episodes=1000, max_steps=200, verbose=True):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation with detailed metrics and timing.\n",
        "    \n",
        "    Args:\n",
        "        checkpoint_path: Path to checkpoint file\n",
        "        env_name: Environment name\n",
        "        num_episodes: Number of test episodes\n",
        "        max_steps: Max steps per episode\n",
        "        verbose: If True, print each episode result\n",
        "    \"\"\"\n",
        "    # Create environment WITHOUT reward shaping for true performance\n",
        "    env = gym.make(env_name)\n",
        "    agent = PPOAgent(env)\n",
        "    agent.load(checkpoint_path)\n",
        "    \n",
        "    # Metrics storage\n",
        "    episode_rewards = []\n",
        "    episode_steps = []\n",
        "    episode_times = []\n",
        "    success_flags = []\n",
        "    timeout_flags = []\n",
        "    \n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"COMPREHENSIVE EVALUATION\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Checkpoint:    {checkpoint_path}\")\n",
        "    print(f\"Environment:   {env_name}\")\n",
        "    print(f\"Episodes:      {num_episodes}\")\n",
        "    print(f\"Max Steps:     {max_steps}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    start_total = time.time()\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        if len(state.shape) == 3:\n",
        "            state = np.transpose(state, (2, 0, 1))\n",
        "        \n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "        \n",
        "        episode_start = time.time()\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            action, _ = agent.select_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            \n",
        "            if len(next_state.shape) == 3:\n",
        "                next_state = np.transpose(next_state, (2, 0, 1))\n",
        "            \n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "        \n",
        "        episode_time = time.time() - episode_start\n",
        "        \n",
        "        # Check success/failure\n",
        "        success = done and info.get('all_boxes_on_target', False)\n",
        "        timeout = steps >= max_steps\n",
        "        \n",
        "        # Record metrics\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_steps.append(steps)\n",
        "        episode_times.append(episode_time)\n",
        "        success_flags.append(1 if success else 0)\n",
        "        timeout_flags.append(1 if timeout else 0)\n",
        "        \n",
        "        # Print individual episode result if verbose\n",
        "        if verbose:\n",
        "            status = \"✓ PASS\" if success else \"✗ FAIL\"\n",
        "            timeout_marker = \"TIMEOUT\" if timeout else \"\"\n",
        "            print(f\"Episode {episode + 1:4d}/{num_episodes} | \"\n",
        "                  f\"Reward: {episode_reward:7.2f} | \"\n",
        "                  f\"Steps: {steps:3d} | \"\n",
        "                  f\"{status:7s} | \"\n",
        "                  f\"{timeout_marker:7s}\")\n",
        "        \n",
        "        # Progress summary every 100 episodes (even if not verbose)\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            current_success_rate = sum(success_flags) / len(success_flags) * 100\n",
        "            avg_reward = np.mean(episode_rewards)\n",
        "            avg_steps = np.mean(episode_steps)\n",
        "            avg_time = np.mean(episode_times)\n",
        "            \n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(f\"PROGRESS UPDATE - Episode {episode + 1}/{num_episodes}\")\n",
        "            print(f\"{'='*80}\")\n",
        "            print(f\"  Success Rate:       {current_success_rate:6.2f}%\")\n",
        "            print(f\"  Avg Reward:         {avg_reward:7.2f}\")\n",
        "            print(f\"  Avg Steps:          {avg_steps:6.1f}\")\n",
        "            print(f\"  Avg Episode Time:   {avg_time:7.4f}s\")\n",
        "            print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    total_time = time.time() - start_total\n",
        "    env.close()\n",
        "    \n",
        "    # Calculate comprehensive statistics\n",
        "    success_rate = sum(success_flags) / num_episodes * 100\n",
        "    timeout_rate = sum(timeout_flags) / num_episodes * 100\n",
        "    \n",
        "    success_episodes = [i for i in range(num_episodes) if success_flags[i] == 1]\n",
        "    failure_episodes = [i for i in range(num_episodes) if success_flags[i] == 0]\n",
        "    \n",
        "    success_rewards = [episode_rewards[i] for i in success_episodes]\n",
        "    failure_rewards = [episode_rewards[i] for i in failure_episodes]\n",
        "    \n",
        "    success_steps_list = [episode_steps[i] for i in success_episodes]\n",
        "    failure_steps_list = [episode_steps[i] for i in failure_episodes]\n",
        "    \n",
        "    # Print comprehensive data overview\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"FINAL EVALUATION SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\n{'OVERALL PERFORMANCE':^80}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"  Total Episodes:          {num_episodes}\")\n",
        "    print(f\"  Successful Episodes:     {len(success_episodes)} ({success_rate:.2f}%)\")\n",
        "    print(f\"  Failed Episodes:         {len(failure_episodes)} ({100-success_rate:.2f}%)\")\n",
        "    print(f\"  Timeout Episodes:        {sum(timeout_flags)} ({timeout_rate:.2f}%)\")\n",
        "    \n",
        "    print(f\"\\n{'REWARD STATISTICS':^80}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"  Overall:\")\n",
        "    print(f\"    Mean:                  {np.mean(episode_rewards):7.2f}\")\n",
        "    print(f\"    Median:                {np.median(episode_rewards):7.2f}\")\n",
        "    print(f\"    Std Dev:               {np.std(episode_rewards):7.2f}\")\n",
        "    print(f\"    Min:                   {min(episode_rewards):7.2f}\")\n",
        "    print(f\"    Max:                   {max(episode_rewards):7.2f}\")\n",
        "    \n",
        "    if success_rewards:\n",
        "        print(f\"  Success Episodes:\")\n",
        "        print(f\"    Mean:                  {np.mean(success_rewards):7.2f}\")\n",
        "        print(f\"    Median:                {np.median(success_rewards):7.2f}\")\n",
        "        print(f\"    Std Dev:               {np.std(success_rewards):7.2f}\")\n",
        "    \n",
        "    if failure_rewards:\n",
        "        print(f\"  Failure Episodes:\")\n",
        "        print(f\"    Mean:                  {np.mean(failure_rewards):7.2f}\")\n",
        "        print(f\"    Median:                {np.median(failure_rewards):7.2f}\")\n",
        "        print(f\"    Std Dev:               {np.std(failure_rewards):7.2f}\")\n",
        "    \n",
        "    print(f\"\\n{'EPISODE DURATION (STEPS)':^80}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"  Overall:\")\n",
        "    print(f\"    Mean:                  {np.mean(episode_steps):7.1f}\")\n",
        "    print(f\"    Median:                {np.median(episode_steps):7.1f}\")\n",
        "    print(f\"    Min:                   {min(episode_steps):7d}\")\n",
        "    print(f\"    Max:                   {max(episode_steps):7d}\")\n",
        "    \n",
        "    if success_steps_list:\n",
        "        print(f\"  Success Episodes:\")\n",
        "        print(f\"    Mean:                  {np.mean(success_steps_list):7.1f}\")\n",
        "        print(f\"    Median:                {np.median(success_steps_list):7.1f}\")\n",
        "        print(f\"    Min:                   {min(success_steps_list):7d}\")\n",
        "        print(f\"    Max:                   {max(success_steps_list):7d}\")\n",
        "    \n",
        "    if failure_steps_list:\n",
        "        print(f\"  Failure Episodes:\")\n",
        "        print(f\"    Mean:                  {np.mean(failure_steps_list):7.1f}\")\n",
        "        print(f\"    Median:                {np.median(failure_steps_list):7.1f}\")\n",
        "    \n",
        "    print(f\"\\n{'TIMING STATISTICS':^80}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"  Avg Episode Time:        {np.mean(episode_times):7.4f}s\")\n",
        "    print(f\"  Total Evaluation Time:   {total_time:7.2f}s\")\n",
        "    print(f\"  Episodes per Second:     {num_episodes/total_time:7.2f}\")\n",
        "    \n",
        "    print(f\"\\n{'QUARTILE ANALYSIS':^80}\")\n",
        "    print(\"-\" * 80)\n",
        "    q1, q2, q3 = np.percentile(episode_rewards, [25, 50, 75])\n",
        "    print(f\"  25th Percentile (Q1):    {q1:7.2f}\")\n",
        "    print(f\"  50th Percentile (Q2):    {q2:7.2f}\")\n",
        "    print(f\"  75th Percentile (Q3):    {q3:7.2f}\")\n",
        "    print(f\"  IQR (Q3 - Q1):           {q3-q1:7.2f}\")\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    return {\n",
        "        'rewards': episode_rewards,\n",
        "        'steps': episode_steps,\n",
        "        'times': episode_times,\n",
        "        'success': success_flags,\n",
        "        'timeout': timeout_flags,\n",
        "        'success_rate': success_rate,\n",
        "        'total_time': total_time\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_individual_graphs(results, checkpoint_name='ep3000'):\n",
        "    \"\"\"\n",
        "    Create 5 separate, presentation-quality graphs for Google Slides.\n",
        "    Each graph is displayed individually for easy copying.\n",
        "    \"\"\"\n",
        "    rewards = results['rewards']\n",
        "    steps = results['steps']\n",
        "    success = results['success']\n",
        "    times = results['times']\n",
        "    \n",
        "    window = 50\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"GENERATING PRESENTATION GRAPHS\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Each graph will be displayed separately for easy copying to slides.\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "    \n",
        "    # ========== GRAPH 1: Average Reward & Success Rate vs Episodes (NEW!) ==========\n",
        "    print(\"Graph 1/5: Average Reward & Success Rate vs Episodes\")\n",
        "    \n",
        "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
        "    \n",
        "    # Calculate rolling averages\n",
        "    rolling_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
        "    rolling_success = np.convolve(success, np.ones(window)/window, mode='valid') * 100\n",
        "    \n",
        "    # Plot reward curve\n",
        "    color1 = 'tab:blue'\n",
        "    ax1.set_xlabel('Episode', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('Average Reward (50-ep window)', color=color1, fontsize=14, fontweight='bold')\n",
        "    line1 = ax1.plot(range(window-1, len(rewards)), rolling_rewards, color=color1, \n",
        "                     linewidth=2.5, label='Avg Reward', alpha=0.9)\n",
        "    ax1.tick_params(axis='y', labelcolor=color1, labelsize=12)\n",
        "    ax1.grid(True, alpha=0.4, linestyle='--')\n",
        "    ax1.set_xlim([0, len(rewards)])\n",
        "    \n",
        "    # Plot success rate on secondary axis\n",
        "    ax2 = ax1.twinx()\n",
        "    color2 = 'tab:orange'\n",
        "    ax2.set_ylabel('Success Rate % (50-ep window)', color=color2, fontsize=14, fontweight='bold')\n",
        "    line2 = ax2.plot(range(window-1, len(success)), rolling_success, color=color2, \n",
        "                     linewidth=2.5, linestyle='--', label='Success Rate', alpha=0.9)\n",
        "    ax2.tick_params(axis='y', labelcolor=color2, labelsize=12)\n",
        "    ax2.set_xlim([0, len(success)])\n",
        "    \n",
        "    # Combined legend\n",
        "    lines = line1 + line2\n",
        "    labels = [l.get_label() for l in lines]\n",
        "    ax1.legend(lines, labels, loc='upper left', fontsize=12, framealpha=0.9)\n",
        "    \n",
        "    plt.title(f'Agent Performance: Reward & Success Rate - {checkpoint_name}', \n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print()\n",
        "    \n",
        "    # ========== GRAPH 2: Average Reward Curve (50-ep Rolling Window) ==========\n",
        "    print(\"Graph 2/5: Average Reward Curve\")\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    \n",
        "    rolling_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
        "    \n",
        "    ax.plot(range(window-1, len(rewards)), rolling_rewards, color='tab:blue', \n",
        "            linewidth=2.5, label=f'{window}-Episode Moving Average')\n",
        "    ax.axhline(y=np.mean(rewards), color='red', linestyle='--', linewidth=2, \n",
        "               label=f'Overall Mean: {np.mean(rewards):.2f}', alpha=0.8)\n",
        "    \n",
        "    ax.set_xlabel('Episode', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Average Reward', fontsize=14, fontweight='bold')\n",
        "    ax.set_title(f'Average Reward Over Time - {checkpoint_name}', \n",
        "                 fontsize=16, fontweight='bold', pad=20)\n",
        "    ax.legend(fontsize=12, framealpha=0.9)\n",
        "    ax.grid(True, alpha=0.4, linestyle='--')\n",
        "    ax.tick_params(labelsize=12)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print()\n",
        "    \n",
        "    # ========== GRAPH 3: Cumulative Success Rate ==========\n",
        "    print(\"Graph 3/5: Cumulative Success Rate\")\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    \n",
        "    cumulative_success = np.cumsum(success) / np.arange(1, len(success) + 1) * 100\n",
        "    \n",
        "    ax.plot(cumulative_success, color='green', linewidth=3, alpha=0.9)\n",
        "    ax.axhline(y=results['success_rate'], color='red', linestyle='--', linewidth=2.5, \n",
        "               label=f\"Final Success Rate: {results['success_rate']:.1f}%\", alpha=0.8)\n",
        "    \n",
        "    ax.set_xlabel('Episode', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Cumulative Success Rate (%)', fontsize=14, fontweight='bold')\n",
        "    ax.set_title(f'Cumulative Success Rate - {checkpoint_name}', \n",
        "                 fontsize=16, fontweight='bold', pad=20)\n",
        "    ax.grid(True, alpha=0.4, linestyle='--')\n",
        "    ax.legend(fontsize=12, framealpha=0.9)\n",
        "    ax.set_ylim([0, max(cumulative_success) * 1.1])\n",
        "    ax.tick_params(labelsize=12)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print()\n",
        "    \n",
        "    # ========== GRAPH 4: Average Episode Duration (Steps) ==========\n",
        "    print(\"Graph 4/5: Average Episode Duration\")\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    \n",
        "    rolling_steps = np.convolve(steps, np.ones(window)/window, mode='valid')\n",
        "    \n",
        "    ax.plot(range(window-1, len(steps)), rolling_steps, color='purple', \n",
        "            linewidth=2.5, label=f'{window}-Episode Moving Average')\n",
        "    ax.axhline(y=np.mean(steps), color='red', linestyle='--', linewidth=2, \n",
        "               label=f'Overall Mean: {np.mean(steps):.1f} steps', alpha=0.8)\n",
        "    \n",
        "    ax.set_xlabel('Episode', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Steps per Episode', fontsize=14, fontweight='bold')\n",
        "    ax.set_title(f'Average Episode Duration - {checkpoint_name}', \n",
        "                 fontsize=16, fontweight='bold', pad=20)\n",
        "    ax.grid(True, alpha=0.4, linestyle='--')\n",
        "    ax.legend(fontsize=12, framealpha=0.9)\n",
        "    ax.tick_params(labelsize=12)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print()\n",
        "    \n",
        "    # ========== GRAPH 5: Performance Metrics Dashboard ==========\n",
        "    print(\"Graph 5/5: Performance Metrics Dashboard\")\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Create comprehensive metrics text\n",
        "    metrics_text = f\"\"\"\n",
        "    PERFORMANCE METRICS SUMMARY\n",
        "    {'=' * 50}\n",
        "    \n",
        "    Checkpoint:             {checkpoint_name}\n",
        "    Episodes Tested:        {len(rewards)}\n",
        "    \n",
        "    {'SUCCESS METRICS':^50}\n",
        "    {'-' * 50}\n",
        "    Success Rate:           {results['success_rate']:.2f}%\n",
        "    Timeout Rate:           {sum(results['timeout'])/len(results['timeout'])*100:.2f}%\n",
        "    \n",
        "    {'REWARD STATISTICS':^50}\n",
        "    {'-' * 50}\n",
        "    Mean Reward:            {np.mean(rewards):7.2f}\n",
        "    Median Reward:          {np.median(rewards):7.2f}\n",
        "    Std Deviation:          {np.std(rewards):7.2f}\n",
        "    Min Reward:             {min(rewards):7.2f}\n",
        "    Max Reward:             {max(rewards):7.2f}\n",
        "    \n",
        "    {'EPISODE DURATION':^50}\n",
        "    {'-' * 50}\n",
        "    Mean Steps:             {np.mean(steps):7.1f}\n",
        "    Median Steps:           {np.median(steps):7.1f}\n",
        "    Min Steps:              {min(steps):7d}\n",
        "    Max Steps:              {max(steps):7d}\n",
        "    \n",
        "    {'TIMING ANALYSIS':^50}\n",
        "    {'-' * 50}\n",
        "    Avg Episode Time:       {np.mean(times):7.4f}s\n",
        "    Total Eval Time:        {results['total_time']:7.2f}s\n",
        "    Episodes/Second:        {len(rewards)/results['total_time']:7.2f}\n",
        "    \n",
        "    {'QUARTILE ANALYSIS':^50}\n",
        "    {'-' * 50}\n",
        "    Q1 (25th percentile):   {np.percentile(rewards, 25):7.2f}\n",
        "    Q2 (50th percentile):   {np.percentile(rewards, 50):7.2f}\n",
        "    Q3 (75th percentile):   {np.percentile(rewards, 75):7.2f}\n",
        "    IQR (Q3 - Q1):          {np.percentile(rewards, 75) - np.percentile(rewards, 25):7.2f}\n",
        "    \"\"\"\n",
        "    \n",
        "    ax.text(0.05, 0.95, metrics_text, transform=ax.transAxes,\n",
        "            fontsize=13, verticalalignment='top', fontfamily='monospace',\n",
        "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8, pad=1.5))\n",
        "    \n",
        "    plt.title(f'Evaluation Summary - {checkpoint_name}', \n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print()\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"ALL GRAPHS GENERATED SUCCESSFULLY!\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nYou can now right-click each graph above to copy/save individually.\")\n",
        "    print(\"Perfect for adding to your Google Slides presentation!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "\n",
        "# Run comprehensive evaluation\n",
        "checkpoint_path = '/content/checkpoints/ppo_sokoban_ep3000.pth'\n",
        "\n",
        "print(\"Starting comprehensive evaluation...\")\n",
        "print(\"This will take several minutes for 1000 episodes.\\n\")\n",
        "\n",
        "results = comprehensive_evaluation(\n",
        "    checkpoint_path, \n",
        "    env_name='Sokoban-small-v0', \n",
        "    num_episodes=1000, \n",
        "    max_steps=200,\n",
        "    verbose=True  # Set to False to hide individual episode output\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Generating presentation-ready graphs...\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "plot_individual_graphs(results, checkpoint_name='ep3000')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comprehensive Evaluation & Visualization (1000 Episodes)\n",
        "\n",
        "This cell will test the ep3000 checkpoint over 1000 episodes and generate 5 presentation-ready graphs:\n",
        "1. Average Reward Curve + Success Rate (%) \n",
        "2. Evaluation Success Rate Over Time\n",
        "3. Average Episode Duration (Steps)\n",
        "4. Reward Distribution Histogram\n",
        "5. Performance Metrics Dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Improved Training Configuration\n",
        "\n",
        "**Key changes to fix policy degradation:**\n",
        "\n",
        "1. **Reduced Reward Shaping (80% reduction)**:\n",
        "   - Distance bonus: 0.5 → **0.1** (completion is now 100x more valuable)\n",
        "   - Box placement: 2.0 → **0.5** (completion is now 20x more valuable)\n",
        "   - **NEW**: Timeout penalty of -3.0 to discourage reward farming\n",
        "\n",
        "2. **Increased Exploration**:\n",
        "   - Entropy coefficient: 0.05 → **0.15** (3x more exploration)\n",
        "   - Prevents getting stuck in local optima\n",
        "\n",
        "**Why these changes help:**\n",
        "- Your agent likely learned to \"farm\" small shaping rewards without completing puzzles\n",
        "- These changes make puzzle completion (+10) much more valuable than intermediate bonuses\n",
        "- Higher entropy forces the agent to explore and rediscover winning strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pygame Visualization: Watch Agent Solve Puzzles\n",
        "\n",
        "This cell provides real-time Pygame visualization of the trained agent solving Sokoban puzzles.\n",
        "\n",
        "**Features:**\n",
        "- Shows 2 episodes: 1 failure and 1 success\n",
        "- Real-time rendering at 5 FPS\n",
        "- 3x scaled display for clarity\n",
        "- Overlay showing episode info, steps, reward, and status\n",
        "- Close window or press ESC to exit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_agent_pygame(checkpoint_path, env_name='Sokoban-small-v0', max_steps=200, fps=5, save_videos=True, output_dir='puzzle_videos', min_success_reward=10.0):\n",
        "    \"\"\"\n",
        "    Visualize trained agent solving Sokoban puzzles using Pygame.\n",
        "    Shows 2 episodes: 1 failure and 1 success (with reward > min_success_reward).\n",
        "    Optionally saves videos to files.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path: Path to trained agent checkpoint\n",
        "        env_name: Sokoban environment name\n",
        "        max_steps: Maximum steps per episode\n",
        "        fps: Frames per second for visualization and video\n",
        "        save_videos: If True, save episodes as video files (MP4)\n",
        "        output_dir: Directory to save videos\n",
        "        min_success_reward: Minimum reward required for success episode (default: 10.0)\n",
        "    \"\"\"\n",
        "    import pygame\n",
        "    import time\n",
        "    import imageio\n",
        "    import os\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Create output directory if saving videos\n",
        "    if save_videos:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        print(f\"Videos will be saved to: {output_dir}/\")\n",
        "\n",
        "    # Create environment WITHOUT reward shaping for true performance\n",
        "    env = gym.make(env_name)\n",
        "    agent = PPOAgent(env)\n",
        "    agent.load(checkpoint_path)\n",
        "\n",
        "    # Initialize Pygame\n",
        "    pygame.init()\n",
        "\n",
        "    # Get initial render to determine window size\n",
        "    obs = env.reset()\n",
        "    img = env.render(mode='rgb_array')\n",
        "    scale = 3\n",
        "    screen = pygame.display.set_mode((img.shape[1] * scale, img.shape[0] * scale))\n",
        "    pygame.display.set_caption('Sokoban Agent Visualization - ep3000')\n",
        "    clock = pygame.time.Clock()\n",
        "\n",
        "    # Font for overlay text\n",
        "    font = pygame.font.Font(None, 32)\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"PYGAME VISUALIZATION STARTED\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Looking for 1 failure and 1 success (reward > {min_success_reward}) to display...\")\n",
        "    print(\"Close window or press ESC to exit\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    episodes_to_show = []\n",
        "    target_outcomes = ['failure', 'success']\n",
        "    attempt_count = 0\n",
        "    max_attempts = 100  # Increased to 100 attempts to find good success episode\n",
        "\n",
        "    # Collect episodes with desired outcomes\n",
        "    while len(episodes_to_show) < 2 and attempt_count < max_attempts:\n",
        "        attempt_count += 1\n",
        "\n",
        "        state = env.reset()\n",
        "        if len(state.shape) == 3:\n",
        "            state = np.transpose(state, (2, 0, 1))\n",
        "\n",
        "        episode_data = {\n",
        "            'frames': [],\n",
        "            'rewards': [],\n",
        "            'steps': 0,\n",
        "            'outcome': None,\n",
        "            'total_reward': 0\n",
        "        }\n",
        "\n",
        "        done = False\n",
        "        steps = 0\n",
        "        episode_reward = 0\n",
        "\n",
        "        # Run episode and collect frames\n",
        "        while not done and steps < max_steps:\n",
        "            # Capture frame\n",
        "            frame = env.render(mode='rgb_array')\n",
        "            episode_data['frames'].append(frame)\n",
        "\n",
        "            # Agent takes action\n",
        "            action, _ = agent.select_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            if len(next_state.shape) == 3:\n",
        "                next_state = np.transpose(next_state, (2, 0, 1))\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "            episode_data['rewards'].append(episode_reward)\n",
        "\n",
        "        # Capture final frame\n",
        "        final_frame = env.render(mode='rgb_array')\n",
        "        episode_data['frames'].append(final_frame)\n",
        "        episode_data['rewards'].append(episode_reward)\n",
        "\n",
        "        # Determine outcome\n",
        "        success = done and info.get('all_boxes_on_target', False)\n",
        "        timeout = steps >= max_steps\n",
        "\n",
        "        episode_data['steps'] = steps\n",
        "        episode_data['total_reward'] = episode_reward\n",
        "        episode_data['outcome'] = 'success' if success else 'failure'\n",
        "\n",
        "        # Check if we need this outcome\n",
        "        needed_outcome = target_outcomes[len(episodes_to_show)]\n",
        "\n",
        "        # For success episodes, also check reward threshold\n",
        "        if needed_outcome == 'success':\n",
        "            if episode_data['outcome'] == 'success' and episode_reward > min_success_reward:\n",
        "                episodes_to_show.append(episode_data)\n",
        "                print(f\"Found {episode_data['outcome']} episode (attempt #{attempt_count}): \"\n",
        "                      f\"Reward={episode_reward:.2f}, Steps={steps} - ACCEPTED (reward > {min_success_reward})\")\n",
        "            elif episode_data['outcome'] == 'success':\n",
        "                print(f\"  Skipping success episode (attempt #{attempt_count}): \"\n",
        "                      f\"Reward={episode_reward:.2f} - TOO LOW (need > {min_success_reward})\")\n",
        "        else:\n",
        "            # For failure episodes, just match the outcome\n",
        "            if episode_data['outcome'] == needed_outcome:\n",
        "                episodes_to_show.append(episode_data)\n",
        "                print(f\"Found {episode_data['outcome']} episode (attempt #{attempt_count}): \"\n",
        "                      f\"Reward={episode_reward:.2f}, Steps={steps}\")\n",
        "\n",
        "    if len(episodes_to_show) < 2:\n",
        "        print(f\"\\nWarning: Could only find {len(episodes_to_show)} episodes after {max_attempts} attempts\")\n",
        "        print(\"Displaying what we found...\")\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"STARTING VISUALIZATION\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    # Display the collected episodes\n",
        "    running = True\n",
        "    for ep_idx, episode_data in enumerate(episodes_to_show):\n",
        "        if not running:\n",
        "            break\n",
        "\n",
        "        episode_num = ep_idx + 1\n",
        "        outcome = episode_data['outcome']\n",
        "        total_steps = episode_data['steps']\n",
        "        total_reward = episode_data['total_reward']\n",
        "\n",
        "        print(f\"Episode {episode_num}/2: {outcome.upper()} \"\n",
        "              f\"(Reward: {total_reward:.2f}, Steps: {total_steps})\")\n",
        "\n",
        "        # Prepare video writer if saving\n",
        "        video_frames = []\n",
        "        if save_videos:\n",
        "            video_filename = f\"{output_dir}/episode_{episode_num}_{outcome}_{timestamp}.mp4\"\n",
        "\n",
        "        # Play through the episode frames\n",
        "        for step_idx, frame in enumerate(episode_data['frames']):\n",
        "            # Check for quit events\n",
        "            for event in pygame.event.get():\n",
        "                if event.type == pygame.QUIT:\n",
        "                    running = False\n",
        "                    break\n",
        "                elif event.type == pygame.KEYDOWN:\n",
        "                    if event.key == pygame.K_ESCAPE:\n",
        "                        running = False\n",
        "                        break\n",
        "\n",
        "            if not running:\n",
        "                break\n",
        "\n",
        "            # Render the game state\n",
        "            surf = pygame.surfarray.make_surface(np.transpose(frame, (1, 0, 2)))\n",
        "            surf = pygame.transform.scale(surf, (frame.shape[1] * scale, frame.shape[0] * scale))\n",
        "            screen.blit(surf, (0, 0))\n",
        "\n",
        "            # Draw semi-transparent overlay background for text\n",
        "            overlay = pygame.Surface((screen.get_width(), 100))\n",
        "            overlay.set_alpha(200)\n",
        "            overlay.fill((0, 0, 0))\n",
        "            screen.blit(overlay, (0, 0))\n",
        "\n",
        "            # Prepare overlay text\n",
        "            current_reward = episode_data['rewards'][step_idx] if step_idx < len(episode_data['rewards']) else total_reward\n",
        "            status = \"SUCCESS\" if outcome == 'success' and step_idx == len(episode_data['frames']) - 1 else                      \"FAILED\" if outcome == 'failure' and step_idx == len(episode_data['frames']) - 1 else                      \"IN PROGRESS\"\n",
        "\n",
        "            # Render text\n",
        "            text_line1 = font.render(f\"Episode: {episode_num}/2  |  Step: {step_idx}/{total_steps}\", True, (255, 255, 255))\n",
        "            text_line2 = font.render(f\"Reward: {current_reward:.2f}  |  Status: {status}\", True, (255, 255, 0))\n",
        "\n",
        "            # Blit text\n",
        "            screen.blit(text_line1, (10, 10))\n",
        "            screen.blit(text_line2, (10, 50))\n",
        "\n",
        "            pygame.display.flip()\n",
        "\n",
        "            # Capture frame for video\n",
        "            if save_videos:\n",
        "                # Get the rendered screen as RGB array\n",
        "                video_frame = pygame.surfarray.array3d(screen)\n",
        "                video_frame = np.transpose(video_frame, (1, 0, 2))  # Transpose to (height, width, channels)\n",
        "                video_frames.append(video_frame)\n",
        "\n",
        "            clock.tick(fps)\n",
        "\n",
        "        # Save video if enabled\n",
        "        if save_videos and video_frames:\n",
        "            print(f\"  Saving video: {video_filename}\")\n",
        "            imageio.mimsave(video_filename, video_frames, fps=fps, codec='libx264', quality=8)\n",
        "            print(f\"  Video saved successfully! ({len(video_frames)} frames)\")\n",
        "\n",
        "        # Pause between episodes (2 seconds)\n",
        "        if running and ep_idx < len(episodes_to_show) - 1:\n",
        "            print(\"  (Pausing 2 seconds before next episode...)\\n\")\n",
        "            time.sleep(2)\n",
        "\n",
        "    # Clean up\n",
        "    pygame.quit()\n",
        "    env.close()\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"VISUALIZATION COMPLETE\")\n",
        "    if save_videos:\n",
        "        print(f\"Videos saved to: {os.path.abspath(output_dir)}/\")\n",
        "    print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Pygame visualization with ep3000 checkpoint\n",
        "checkpoint_path = r'c:\\Users\\canid\\OneDrive\\Masaüstü\\Python\\CS_175\\sokobanRL\\google_colab_checkpoints\\sokoban-small-v0\\ppo_sokoban_ep3000.pth'\n",
        "\n",
        "print(\"Loading checkpoint and starting Pygame visualization...\")\n",
        "print(\"This will show 2 episodes: 1 failure and 1 success (reward > 10)\")\n",
        "print(\"Videos will be saved to puzzle_videos/ folder\")\n",
        "print(\"Close the Pygame window or press ESC to exit\\n\")\n",
        "\n",
        "visualize_agent_pygame(\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    env_name='Sokoban-small-v0',\n",
        "    max_steps=200,\n",
        "    fps=5,  # 5 frames per second for easy viewing\n",
        "    save_videos=True,  # Set to False to disable video saving\n",
        "    output_dir='puzzle_videos',  # Folder where videos will be saved\n",
        "    min_success_reward=9.0  # Only show success episodes with reward > 10\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
